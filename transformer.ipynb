{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ruff in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from -r tic-tac-toe/requirements.txt (line 1)) (0.9.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from -r tic-tac-toe/requirements.txt (line 2)) (75.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from -r tic-tac-toe/requirements.txt (line 5)) (2.2.3)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from -r tic-tac-toe/requirements.txt (line 6)) (6.29.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (1.8.11)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (8.32.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (6.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (5.14.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (5.2.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (308)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\spencer\\anaconda3\\envs\\tic\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r tic-tac-toe/requirements.txt (line 6)) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Obtaining file:///C:/Users/Spencer/Documents/CUBoulder/CSCI7000/Assignment1/tic-tac-toe\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: tictactoe\n",
      "  Building editable for tictactoe (pyproject.toml): started\n",
      "  Building editable for tictactoe (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for tictactoe: filename=tictactoe-0.0.1-0.editable-py3-none-any.whl size=2824 sha256=ff40b4e98f2ff73fc762ad2aabb18d8d62a656459ca08e5825e514ace5b6c268\n",
      "  Stored in directory: C:\\Users\\Spencer\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-76c2cwem\\wheels\\d6\\5a\\8d\\feebe8c7295a76526ef50a01375931f6375dbc01f17aab1db7\n",
      "Successfully built tictactoe\n",
      "Installing collected packages: tictactoe\n",
      "  Attempting uninstall: tictactoe\n",
      "    Found existing installation: tictactoe 0.0.1\n",
      "    Uninstalling tictactoe-0.0.1:\n",
      "      Successfully uninstalled tictactoe-0.0.1\n",
      "Successfully installed tictactoe-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r tic-tac-toe/requirements.txt\n",
    "%pip install -e tic-tac-toe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tictactoe.board import Board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "- Create Data\n",
    "    - Create Recursive function to get all board positions\n",
    "    - Create Function to get best move\n",
    "- Create Transformer\n",
    "- Train Transformer\n",
    "- Write article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 1 j: 0 board:[[ 0  1  1]\n",
      " [-1 -1  0]\n",
      " [-1 -1  1]]\n",
      "i: 2 j: 2 board:[[ 0  1  1]\n",
      " [ 0 -1  0]\n",
      " [-1 -1  1]]\n",
      "i: 1 j: 1 board:[[ 0  1  1]\n",
      " [ 0 -1  0]\n",
      " [-1 -1  0]]\n",
      "i: 0 j: 1 board:[[ 0  1  1]\n",
      " [ 0  0  0]\n",
      " [-1 -1  0]]\n",
      "i: 2 j: 0 board:[[ 0  0  1]\n",
      " [ 0  0  0]\n",
      " [-1 -1  0]]\n",
      "i: 0 j: 2 board:[[ 0  0  1]\n",
      " [ 0  0  0]\n",
      " [ 0 -1  0]]\n",
      "i: 2 j: 1 board:[[ 0  0  0]\n",
      " [ 0  0  0]\n",
      " [ 0 -1  0]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[357], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m j: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m board:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_board\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mgenerate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[357], line 22\u001b[0m, in \u001b[0;36mgenerate_data\u001b[1;34m(board)\u001b[0m\n\u001b[0;32m     20\u001b[0m new_board \u001b[38;5;241m=\u001b[39m nb\u001b[38;5;241m.\u001b[39mboard\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mgenerate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_board\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m j: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m board:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_board\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[357], line 22\u001b[0m, in \u001b[0;36mgenerate_data\u001b[1;34m(board)\u001b[0m\n\u001b[0;32m     20\u001b[0m new_board \u001b[38;5;241m=\u001b[39m nb\u001b[38;5;241m.\u001b[39mboard\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mgenerate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_board\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m j: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m board:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_board\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[1;31m[... skipping similar frames: generate_data at line 22 (4 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[357], line 22\u001b[0m, in \u001b[0;36mgenerate_data\u001b[1;34m(board)\u001b[0m\n\u001b[0;32m     20\u001b[0m new_board \u001b[38;5;241m=\u001b[39m nb\u001b[38;5;241m.\u001b[39mboard\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mgenerate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_board\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m j: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m board:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_board\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[357], line 5\u001b[0m, in \u001b[0;36mgenerate_data\u001b[1;34m(board)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_data\u001b[39m(board):\n\u001b[0;32m      4\u001b[0m     b \u001b[38;5;241m=\u001b[39m Board()\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray2string(\u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(boards\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(board \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m      6\u001b[0m         out_boards \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b\u001b[38;5;241m.\u001b[39mget_best_moves(board)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "size = 3\n",
    "boards = {}\n",
    "def generate_data(board):\n",
    "    b = Board()\n",
    "    if np.array2string(board.flatten()) not in list(boards.keys()):\n",
    "        out_boards = []\n",
    "        for i,j in b.get_best_moves(board):\n",
    "            nb = Board()\n",
    "            nb.set_board(np.copy(board))\n",
    "            nb.place(i,j)\n",
    "            out_boards.append(nb.to_str())\n",
    "        boards[np.array2string(board.flatten())] = out_boards\n",
    "\n",
    "    for i, j in list(zip(*np.where(board==0))):\n",
    "        nb = Board()\n",
    "        nb.set_board(np.copy(board))\n",
    "        nb.place(i,j)\n",
    "        new_board = nb.board\n",
    "        try:\n",
    "            generate_data(new_board)\n",
    "        except:\n",
    "            print(f\"i: {i} j: {j} board:{new_board}\")\n",
    "            raise\n",
    "generate_data(np.array([[0,0,0],\n",
    "                        [0,0,0],\n",
    "                        [0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_data = {k:v for k, v in data.items() if v != []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6046"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(boards.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex_key: [0 0 0 0 0 0 0 0 0] \n",
      " ex_vals: ['[-1  0  0  0  0  0  0  0  0]', '[ 0 -1  0  0  0  0  0  0  0]', '[ 0  0 -1  0  0  0  0  0  0]', '[ 0  0  0 -1  0  0  0  0  0]', '[ 0  0  0  0 -1  0  0  0  0]', '[ 0  0  0  0  0 -1  0  0  0]', '[ 0  0  0  0  0  0 -1  0  0]', '[ 0  0  0  0  0  0  0 -1  0]', '[ 0  0  0  0  0  0  0  0 -1]']\n"
     ]
    }
   ],
   "source": [
    "ex_key = list(boards.keys())[0]\n",
    "print(f\"ex_key: {ex_key} \\n ex_vals: {boards[ex_key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./data/data.json', 'w') as f:\n",
    "    json.dump(fixed_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5920"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_percent, val_percent, test_percent = 0.7, 0.2, 0.1\n",
    "\n",
    "data_keys = list(data.keys())\n",
    "n = len(data_keys)\n",
    "num_train, num_val, num_test = int(train_percent*n), int(val_percent*n), int(test_percent*n)\n",
    "\n",
    "select_train = random.sample(data_keys, num_train)\n",
    "val_and_test = [value for value in data_keys if value not in select_train]\n",
    "\n",
    "select_val = random.sample(val_and_test, num_val)\n",
    "select_test = [value for value in val_and_test if value not in select_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boards_to_format(key):\n",
    "    key = key[1:-1]\n",
    "    return [int(c) for c in key.split(' ') if c in ['-1', '0', '1']]\n",
    "\n",
    "train_data = {int(i): (boards_to_format(k), [boards_to_format(v) for v in data[k]]) for i, k in enumerate(select_train)}\n",
    "val_data = {int(i): (boards_to_format(k), [boards_to_format(v) for v in data[k]]) for i, k in enumerate(select_val)}\n",
    "test_data = {int(i): (boards_to_format(k), [boards_to_format(v) for v in data[k]]) for i, k in enumerate(select_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4144\n",
      "1184\n",
      "592\n"
     ]
    }
   ],
   "source": [
    "print(len(list(train_data.keys())))\n",
    "print(len(list(val_data.keys())))\n",
    "print(len(list(test_data.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train.json', 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "with open('./data/val.json', 'w') as f:\n",
    "    json.dump(val_data, f)\n",
    "with open('./data/test.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate Data into dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class TicTacDataset(Dataset):\n",
    "    def __init__(self, path = './data/train.json'):\n",
    "        with open(path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.len = len(list(self.data.keys()))\n",
    "        \n",
    "        # self.inputs, self.outputs = [], []\n",
    "        # for ins, outs in data.items():\n",
    "        #     for out in outs:\n",
    "        #         self.inputs.append(self.boards_to_format(ins))\n",
    "        #         self.outputs.append(self.boards_to_format(out))\n",
    "\n",
    "    # def boards_to_format(self, key):\n",
    "    #     key = key[1:-1]\n",
    "    #     return [int(c) for c in key.split(' ') if c in ['-1', '0', '1']]\n",
    "        \n",
    "    def __len__(self,):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ins, outs = self.data[str(idx)]\n",
    "        return ins, outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TicTacDataset(path='./data/train.json')\n",
    "val_dataset = TicTacDataset(path='./data/val.json')\n",
    "test_dataset = TicTacDataset(path='./data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    # print(batch)\n",
    "    ins = [torch.tensor(b[0])+1 for b in batch]\n",
    "    # outs = [torch.tensor(np.average(np.array(b[1])+1, axis=0)) for b in batch]\n",
    "    outs = [torch.tensor(b[1][0])+1 for b in batch]\n",
    "    # print(outs)\n",
    "\n",
    "\n",
    "\n",
    "    # max_outs = max([len(o) for o in outs])\n",
    "    # padded_outs = []\n",
    "    # for o in outs:\n",
    "    #   pad_boxes = max_outs - len(o)\n",
    "    #   padded_out = (o) + [[1,1,1,1,1,1,1,1,1]] * pad_boxes\n",
    "    #   padded_outs.append(torch.tensor(padded_out, dtype=torch.float32)+1)\n",
    "    return torch.stack(ins), torch.stack(outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set default device to CUDA if available, otherwise CPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda')\n",
    "else:\n",
    "    torch.set_default_device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 3\n",
    "num_embedding = 32\n",
    "\n",
    "num_heads = 8\n",
    "\n",
    "num_blocks = 4\n",
    "block_size = 9\n",
    "\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" Singular Head\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(num_embedding, head_size, bias=False)\n",
    "        self.query = nn.Linear(num_embedding, head_size, bias=False)\n",
    "        self.value = nn.Linear(num_embedding, head_size, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        k = self.key(x) # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "\n",
    "        # Computer attention\n",
    "        # C**0.5 is to make the numbers smaller so softmax doesn't do weird things\n",
    "        wei = q @ k.transpose(-2, -1) / C**0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # \n",
    "        V = self.value(x)\n",
    "        out = wei @ V # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple Attention Heads\"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for i in range(num_heads)])\n",
    "        self.project = nn.Linear(num_embedding, num_embedding)                  # Projection layer for gettting back into the residual pathway\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.project(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"Single Layer\"\"\"\n",
    "\n",
    "    def __init__(self, num_embedding):\n",
    "        super().__init__()\n",
    "\n",
    "        self.m = nn.Sequential(nn.Linear(num_embedding, 4 * num_embedding), # 4* because they did it in the paper\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(4 * num_embedding, num_embedding), # Projection layer for gettting back into the residual pathway\n",
    "                               nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, num_embedding, num_head):\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = num_embedding // num_head\n",
    "        self.self_attn = MultiHeadAttention(num_head, head_size)\n",
    "        self.feed_fwd = FeedFoward(num_embedding)\n",
    "\n",
    "        self.lay_norm1 = nn.LayerNorm(num_embedding)\n",
    "        self.lay_norm2 = nn.LayerNorm(num_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attn(self.lay_norm1(x))\n",
    "        x = x + self.feed_fwd(self.lay_norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,num_embedding)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, num_embedding)\n",
    "\n",
    "        # Attention\n",
    "        self.attn_blocks = nn.Sequential(\n",
    "            *[Block(num_embedding, num_head = 4) for i in range(num_blocks)],\n",
    "            nn.LayerNorm(num_embedding),\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(num_embedding, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        B, T = inputs.shape\n",
    "        # print(inputs[0])\n",
    "        # print(labels[0])\n",
    "\n",
    "        token_embedding = self.token_embedding_table(inputs) # (B, T, C)\n",
    "        position_embedding = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
    "        x = token_embedding + position_embedding # (B, T, C)\n",
    "\n",
    "        x = self.attn_blocks(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # print(logits.shape)\n",
    "            # print(labels.shape)\n",
    "            \n",
    "            # loss = F.cross_entropy(logits, labels) # need to add \n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.4243710041046143\n",
      "Training Loss: 1.2248929738998413\n",
      "Training Loss: 1.1514091491699219\n",
      "Training Loss: 1.0253156423568726\n",
      "Training Loss: 0.9188567399978638\n",
      "Training Loss: 0.8241943120956421\n",
      "Training Loss: 0.7607859969139099\n",
      "Training Loss: 0.685420036315918\n",
      "Training Loss: 0.624018669128418\n",
      "Training Loss: 0.5655301809310913\n",
      "Training Loss: 0.5146426558494568\n",
      "Training Loss: 0.47928163409233093\n",
      "Training Loss: 0.4522138833999634\n",
      "Training Loss: 0.41116735339164734\n",
      "Training Loss: 0.39550676941871643\n",
      "Training Loss: 0.38081228733062744\n",
      "Training Loss: 0.3689066767692566\n",
      "Training Loss: 0.352457731962204\n",
      "Training Loss: 0.34605658054351807\n",
      "Training Loss: 0.3385683000087738\n",
      "Training Loss: 0.3335327208042145\n",
      "Training Loss: 0.32615941762924194\n",
      "Training Loss: 0.31354615092277527\n",
      "Training Loss: 0.3108419179916382\n",
      "Training Loss: 0.32639262080192566\n",
      "Training Loss: 0.3128700852394104\n",
      "Training Loss: 0.301074355840683\n",
      "Training Loss: 0.3137728273868561\n",
      "Training Loss: 0.29779481887817383\n",
      "Training Loss: 0.29472166299819946\n",
      "Training Loss: 0.30016204714775085\n",
      "Training Loss: 0.29495158791542053\n",
      "Training Loss: 0.29873600602149963\n",
      "Training Loss: 0.3016297221183777\n",
      "Training Loss: 0.29562118649482727\n",
      "Training Loss: 0.28801995515823364\n",
      "Training Loss: 0.2815510630607605\n",
      "Training Loss: 0.2703382670879364\n",
      "Training Loss: 0.2600468397140503\n",
      "Training Loss: 0.26771581172943115\n",
      "Training Loss: 0.2789369821548462\n",
      "Training Loss: 0.2654200494289398\n",
      "Training Loss: 0.27250948548316956\n",
      "Training Loss: 0.26488474011421204\n",
      "Training Loss: 0.2572324573993683\n",
      "Training Loss: 0.28048640489578247\n",
      "Training Loss: 0.26275739073753357\n",
      "Training Loss: 0.24473033845424652\n",
      "Training Loss: 0.2568973898887634\n",
      "Training Loss: 0.27955329418182373\n",
      "Training Loss: 0.2531459331512451\n",
      "Training Loss: 0.26201140880584717\n",
      "Training Loss: 0.2514043152332306\n",
      "Training Loss: 0.2691238522529602\n",
      "Training Loss: 0.23876431584358215\n",
      "Training Loss: 0.24368473887443542\n",
      "Training Loss: 0.2562001943588257\n",
      "Training Loss: 0.25940823554992676\n",
      "Training Loss: 0.234455406665802\n",
      "Training Loss: 0.22130823135375977\n",
      "Training Loss: 0.2595120072364807\n",
      "Training Loss: 0.2370879352092743\n",
      "Training Loss: 0.2638009488582611\n",
      "Training Loss: 0.23757730424404144\n",
      "Training Loss: 0.23921231925487518\n",
      "Training Loss: 0.23199523985385895\n",
      "Training Loss: 0.21016408503055573\n",
      "Training Loss: 0.22193312644958496\n",
      "Training Loss: 0.2276829332113266\n",
      "Training Loss: 0.22312313318252563\n",
      "Training Loss: 0.21486696600914001\n",
      "Training Loss: 0.24114929139614105\n",
      "Training Loss: 0.22291827201843262\n",
      "Training Loss: 0.21840491890907288\n",
      "Training Loss: 0.20340600609779358\n",
      "Training Loss: 0.19020989537239075\n",
      "Training Loss: 0.21273104846477509\n",
      "Training Loss: 0.20370261371135712\n",
      "Training Loss: 0.20077933371067047\n",
      "Training Loss: 0.2299797534942627\n",
      "Training Loss: 0.22463983297348022\n",
      "Training Loss: 0.2096247375011444\n",
      "Training Loss: 0.22311390936374664\n",
      "Training Loss: 0.19329391419887543\n",
      "Training Loss: 0.20838484168052673\n",
      "Training Loss: 0.18743644654750824\n",
      "Training Loss: 0.2055083066225052\n",
      "Training Loss: 0.2129214107990265\n",
      "Training Loss: 0.22688817977905273\n",
      "Training Loss: 0.1907612830400467\n",
      "Training Loss: 0.20422141253948212\n",
      "Training Loss: 0.1813308745622635\n",
      "Training Loss: 0.2080976516008377\n",
      "Training Loss: 0.20917700231075287\n",
      "Training Loss: 0.22742226719856262\n",
      "Training Loss: 0.1877254992723465\n",
      "Training Loss: 0.18114954233169556\n",
      "Training Loss: 0.2034505009651184\n",
      "Training Loss: 0.19422486424446106\n",
      "Training Loss: 0.20877346396446228\n",
      "Training Loss: 0.21659742295742035\n",
      "Training Loss: 0.21876177191734314\n",
      "Training Loss: 0.23075324296951294\n",
      "Training Loss: 0.19310736656188965\n",
      "Training Loss: 0.17800964415073395\n",
      "Training Loss: 0.18350070714950562\n",
      "Training Loss: 0.1840619444847107\n",
      "Training Loss: 0.1714944839477539\n",
      "Training Loss: 0.18162280321121216\n",
      "Training Loss: 0.21397659182548523\n",
      "Training Loss: 0.19276505708694458\n",
      "Training Loss: 0.17323428392410278\n",
      "Training Loss: 0.1723923683166504\n",
      "Training Loss: 0.18608802556991577\n",
      "Training Loss: 0.1980818510055542\n",
      "Training Loss: 0.18419481813907623\n",
      "Training Loss: 0.21464286744594574\n",
      "Training Loss: 0.21477670967578888\n",
      "Training Loss: 0.18276892602443695\n",
      "Training Loss: 0.179568350315094\n",
      "Training Loss: 0.19863492250442505\n",
      "Training Loss: 0.16204240918159485\n",
      "Training Loss: 0.18651555478572845\n",
      "Training Loss: 0.1986953765153885\n",
      "Training Loss: 0.1914445161819458\n",
      "Training Loss: 0.17969928681850433\n",
      "Training Loss: 0.1790628731250763\n",
      "Training Loss: 0.193132683634758\n",
      "Training Loss: 0.19521552324295044\n",
      "Training Loss: 0.18749190866947174\n",
      "Training Loss: 0.18505407869815826\n",
      "Training Loss: 0.1704152226448059\n",
      "Training Loss: 0.16521711647510529\n",
      "Training Loss: 0.18985800445079803\n",
      "Training Loss: 0.19962745904922485\n",
      "Training Loss: 0.18480302393436432\n",
      "Training Loss: 0.17031987011432648\n",
      "Training Loss: 0.19235530495643616\n",
      "Training Loss: 0.20032861828804016\n",
      "Training Loss: 0.18590962886810303\n",
      "Training Loss: 0.1780574917793274\n",
      "Training Loss: 0.16551102697849274\n",
      "Training Loss: 0.19713811576366425\n",
      "Training Loss: 0.1826643943786621\n",
      "Training Loss: 0.18876035511493683\n",
      "Training Loss: 0.1532709002494812\n",
      "Training Loss: 0.18430671095848083\n",
      "Training Loss: 0.1936323046684265\n",
      "Training Loss: 0.18511122465133667\n",
      "Training Loss: 0.18996268510818481\n",
      "Training Loss: 0.18981631100177765\n",
      "Training Loss: 0.1738639771938324\n",
      "Training Loss: 0.16026437282562256\n",
      "Training Loss: 0.1767132580280304\n",
      "Training Loss: 0.1887844353914261\n",
      "Training Loss: 0.17731305956840515\n",
      "Training Loss: 0.17494015395641327\n",
      "Training Loss: 0.20161950588226318\n",
      "Training Loss: 0.16012920439243317\n",
      "Training Loss: 0.21340304613113403\n",
      "Training Loss: 0.16731393337249756\n",
      "Training Loss: 0.1875518262386322\n",
      "Training Loss: 0.1896071434020996\n",
      "Training Loss: 0.1919843852519989\n",
      "Training Loss: 0.19106291234493256\n",
      "Training Loss: 0.15356750786304474\n",
      "Training Loss: 0.16434068977832794\n",
      "Training Loss: 0.19206996262073517\n",
      "Training Loss: 0.21428227424621582\n",
      "Training Loss: 0.17494122684001923\n",
      "Training Loss: 0.2076674848794937\n",
      "Training Loss: 0.1579532027244568\n",
      "Training Loss: 0.18535944819450378\n",
      "Training Loss: 0.17587703466415405\n",
      "Training Loss: 0.19105014204978943\n",
      "Training Loss: 0.16075776517391205\n",
      "Training Loss: 0.17963317036628723\n",
      "Training Loss: 0.18245188891887665\n",
      "Training Loss: 0.1797993779182434\n",
      "Training Loss: 0.2099151313304901\n",
      "Training Loss: 0.1835855096578598\n",
      "Training Loss: 0.1595829725265503\n",
      "Training Loss: 0.14985843002796173\n",
      "Training Loss: 0.16151712834835052\n",
      "Training Loss: 0.1627531349658966\n",
      "Training Loss: 0.15550047159194946\n",
      "Training Loss: 0.15993031859397888\n",
      "Training Loss: 0.16546788811683655\n",
      "Training Loss: 0.14776630699634552\n",
      "Training Loss: 0.1582724004983902\n",
      "Training Loss: 0.20296376943588257\n",
      "Training Loss: 0.191753551363945\n",
      "Training Loss: 0.1614435762166977\n",
      "Training Loss: 0.16750787198543549\n",
      "Training Loss: 0.21805204451084137\n",
      "Training Loss: 0.15460127592086792\n",
      "Training Loss: 0.18552550673484802\n",
      "Training Loss: 0.17983761429786682\n",
      "Training Loss: 0.15445201098918915\n",
      "Training Loss: 0.15666905045509338\n",
      "Training Loss: 0.17110233008861542\n",
      "Training Loss: 0.1657618284225464\n",
      "Training Loss: 0.17440249025821686\n",
      "Training Loss: 0.16375631093978882\n",
      "Training Loss: 0.17223992943763733\n",
      "Training Loss: 0.17907491326332092\n",
      "Training Loss: 0.15741027891635895\n",
      "Training Loss: 0.16564030945301056\n",
      "Training Loss: 0.1765740066766739\n",
      "Training Loss: 0.15927182137966156\n",
      "Training Loss: 0.2080697864294052\n",
      "Training Loss: 0.17447397112846375\n",
      "Training Loss: 0.16213268041610718\n",
      "Training Loss: 0.18141323328018188\n",
      "Training Loss: 0.1783856749534607\n",
      "Training Loss: 0.15560637414455414\n",
      "Training Loss: 0.187880739569664\n",
      "Training Loss: 0.177673801779747\n",
      "Training Loss: 0.18271051347255707\n",
      "Training Loss: 0.1632213145494461\n",
      "Training Loss: 0.16410523653030396\n",
      "Training Loss: 0.17624425888061523\n",
      "Training Loss: 0.18762187659740448\n",
      "Training Loss: 0.19392579793930054\n",
      "Training Loss: 0.16312433779239655\n",
      "Training Loss: 0.18421192467212677\n",
      "Training Loss: 0.1644260436296463\n",
      "Training Loss: 0.1745079606771469\n",
      "Training Loss: 0.17166996002197266\n",
      "Training Loss: 0.16744042932987213\n",
      "Training Loss: 0.17706656455993652\n",
      "Training Loss: 0.20775456726551056\n",
      "Training Loss: 0.17844416201114655\n",
      "Training Loss: 0.17459222674369812\n",
      "Training Loss: 0.17886675894260406\n",
      "Training Loss: 0.2082023024559021\n",
      "Training Loss: 0.15481708943843842\n",
      "Training Loss: 0.1663009375333786\n",
      "Training Loss: 0.17868837714195251\n",
      "Training Loss: 0.1642577052116394\n",
      "Training Loss: 0.1849326491355896\n",
      "Training Loss: 0.15302921831607819\n",
      "Training Loss: 0.13138309121131897\n",
      "Training Loss: 0.15630480647087097\n",
      "Training Loss: 0.1916765719652176\n",
      "Training Loss: 0.18857742846012115\n",
      "Training Loss: 0.1806587427854538\n",
      "Training Loss: 0.16086390614509583\n",
      "Training Loss: 0.16391152143478394\n",
      "Training Loss: 0.1661411076784134\n",
      "Training Loss: 0.18461719155311584\n",
      "Training Loss: 0.16308251023292542\n",
      "Training Loss: 0.1821403205394745\n",
      "Training Loss: 0.21193578839302063\n",
      "Training Loss: 0.14473988115787506\n",
      "Training Loss: 0.1803714632987976\n",
      "Training Loss: 0.16672299802303314\n",
      "Training Loss: 0.16363954544067383\n",
      "Training Loss: 0.15617235004901886\n",
      "Training Loss: 0.1809709668159485\n",
      "Training Loss: 0.1806757003068924\n",
      "Training Loss: 0.191034734249115\n",
      "Training Loss: 0.19064894318580627\n",
      "Training Loss: 0.15437200665473938\n",
      "Training Loss: 0.15939867496490479\n",
      "Training Loss: 0.13669069111347198\n",
      "Training Loss: 0.1950799524784088\n",
      "Training Loss: 0.16864435374736786\n",
      "Training Loss: 0.1664763242006302\n",
      "Training Loss: 0.1972937136888504\n",
      "Training Loss: 0.15629960596561432\n",
      "Training Loss: 0.1522727608680725\n",
      "Training Loss: 0.1563108116388321\n",
      "Training Loss: 0.16587725281715393\n",
      "Training Loss: 0.18744540214538574\n",
      "Training Loss: 0.15271121263504028\n",
      "Training Loss: 0.17425432801246643\n",
      "Training Loss: 0.13919392228126526\n",
      "Training Loss: 0.17399512231349945\n",
      "Training Loss: 0.15101690590381622\n",
      "Training Loss: 0.18239104747772217\n",
      "Training Loss: 0.17354440689086914\n",
      "Training Loss: 0.1631319522857666\n",
      "Training Loss: 0.1896050125360489\n",
      "Training Loss: 0.13182780146598816\n",
      "Training Loss: 0.1522846221923828\n",
      "Training Loss: 0.1765216886997223\n",
      "Training Loss: 0.1559998095035553\n",
      "Training Loss: 0.18493002653121948\n",
      "Training Loss: 0.17413780093193054\n",
      "Training Loss: 0.16596496105194092\n",
      "Training Loss: 0.15330751240253448\n",
      "Training Loss: 0.1470353901386261\n",
      "Training Loss: 0.18393760919570923\n",
      "Training Loss: 0.1690334975719452\n",
      "Training Loss: 0.18318894505500793\n",
      "Training Loss: 0.19808131456375122\n",
      "Training Loss: 0.16741535067558289\n",
      "Training Loss: 0.15766531229019165\n",
      "Training Loss: 0.18193167448043823\n",
      "Training Loss: 0.16526667773723602\n",
      "Training Loss: 0.16959641873836517\n",
      "Training Loss: 0.179172545671463\n",
      "Training Loss: 0.16213415563106537\n",
      "Training Loss: 0.17534424364566803\n",
      "Training Loss: 0.1759570986032486\n",
      "Training Loss: 0.15865272283554077\n",
      "Training Loss: 0.1798122674226761\n",
      "Training Loss: 0.18351583182811737\n",
      "Training Loss: 0.18843843042850494\n",
      "Training Loss: 0.15570832788944244\n",
      "Training Loss: 0.17029614746570587\n",
      "Training Loss: 0.19478625059127808\n",
      "Training Loss: 0.15044313669204712\n",
      "Training Loss: 0.16062898933887482\n",
      "Training Loss: 0.18401306867599487\n",
      "Training Loss: 0.16717542707920074\n",
      "Training Loss: 0.16319087147712708\n",
      "Training Loss: 0.16392888128757477\n",
      "Training Loss: 0.1595035344362259\n",
      "Training Loss: 0.17788414657115936\n",
      "Training Loss: 0.15791527926921844\n",
      "Training Loss: 0.17163540422916412\n",
      "Training Loss: 0.17211134731769562\n",
      "Training Loss: 0.19912441074848175\n",
      "Training Loss: 0.1509941816329956\n",
      "Training Loss: 0.18353253602981567\n",
      "Training Loss: 0.1676657348871231\n",
      "Training Loss: 0.15850648283958435\n",
      "Training Loss: 0.1320607215166092\n",
      "Training Loss: 0.16581439971923828\n",
      "Training Loss: 0.15022510290145874\n",
      "Training Loss: 0.13859441876411438\n",
      "Training Loss: 0.1558217853307724\n",
      "Training Loss: 0.17956526577472687\n",
      "Training Loss: 0.15614619851112366\n",
      "Training Loss: 0.16214603185653687\n",
      "Training Loss: 0.17232553660869598\n",
      "Training Loss: 0.1747729331254959\n",
      "Training Loss: 0.15528595447540283\n",
      "Training Loss: 0.16671372950077057\n",
      "Training Loss: 0.20115546882152557\n",
      "Training Loss: 0.14252308011054993\n",
      "Training Loss: 0.14126330614089966\n",
      "Training Loss: 0.1582900434732437\n",
      "Training Loss: 0.20156949758529663\n",
      "Training Loss: 0.1892135739326477\n",
      "Training Loss: 0.16913656890392303\n",
      "Training Loss: 0.14368170499801636\n",
      "Training Loss: 0.17772096395492554\n",
      "Training Loss: 0.15333740413188934\n",
      "Training Loss: 0.18061187863349915\n",
      "Training Loss: 0.16535724699497223\n",
      "Training Loss: 0.22861000895500183\n",
      "Training Loss: 0.17113180458545685\n",
      "Training Loss: 0.18217779695987701\n",
      "Training Loss: 0.15005284547805786\n",
      "Training Loss: 0.1667991280555725\n",
      "Training Loss: 0.14767831563949585\n",
      "Training Loss: 0.15947991609573364\n",
      "Training Loss: 0.16131789982318878\n",
      "Training Loss: 0.173048198223114\n",
      "Training Loss: 0.19147098064422607\n",
      "Training Loss: 0.1301085203886032\n",
      "Training Loss: 0.17592395842075348\n",
      "Training Loss: 0.15834054350852966\n",
      "Training Loss: 0.1463141292333603\n",
      "Training Loss: 0.18785616755485535\n",
      "Training Loss: 0.1572801023721695\n",
      "Training Loss: 0.16603052616119385\n",
      "Training Loss: 0.15586213767528534\n",
      "Training Loss: 0.17053596675395966\n",
      "Training Loss: 0.17599806189537048\n",
      "Training Loss: 0.20921847224235535\n",
      "Training Loss: 0.16776691377162933\n",
      "Training Loss: 0.18694311380386353\n",
      "Training Loss: 0.16951102018356323\n",
      "Training Loss: 0.16137386858463287\n",
      "Training Loss: 0.1433374434709549\n",
      "Training Loss: 0.16893061995506287\n",
      "Training Loss: 0.17979973554611206\n",
      "Training Loss: 0.1722278594970703\n",
      "Training Loss: 0.1632670760154724\n",
      "Training Loss: 0.1505279242992401\n",
      "Training Loss: 0.18164017796516418\n",
      "Training Loss: 0.15483790636062622\n",
      "Training Loss: 0.1904277801513672\n",
      "Training Loss: 0.1767776757478714\n",
      "Training Loss: 0.16558192670345306\n",
      "Training Loss: 0.1645861715078354\n",
      "Training Loss: 0.17559809982776642\n",
      "Training Loss: 0.14996226131916046\n",
      "Training Loss: 0.18252433836460114\n",
      "Training Loss: 0.15979599952697754\n",
      "Training Loss: 0.14187613129615784\n",
      "Training Loss: 0.13205577433109283\n",
      "Training Loss: 0.13801580667495728\n",
      "Training Loss: 0.14371991157531738\n",
      "Training Loss: 0.1889215111732483\n",
      "Training Loss: 0.17147423326969147\n",
      "Training Loss: 0.18727198243141174\n",
      "Training Loss: 0.14708052575588226\n",
      "Training Loss: 0.19132362306118011\n",
      "Training Loss: 0.16847455501556396\n",
      "Training Loss: 0.1478138566017151\n",
      "Training Loss: 0.15165512263774872\n",
      "Training Loss: 0.1856522113084793\n",
      "Training Loss: 0.1956355720758438\n",
      "Training Loss: 0.18604518473148346\n",
      "Training Loss: 0.16922269761562347\n",
      "Training Loss: 0.1490742266178131\n",
      "Training Loss: 0.1851222962141037\n",
      "Training Loss: 0.1664627194404602\n",
      "Training Loss: 0.16505080461502075\n",
      "Training Loss: 0.1545533537864685\n",
      "Training Loss: 0.16612501442432404\n",
      "Training Loss: 0.1612168252468109\n",
      "Training Loss: 0.18111085891723633\n",
      "Training Loss: 0.15971165895462036\n",
      "Training Loss: 0.18546521663665771\n",
      "Training Loss: 0.17987528443336487\n",
      "Training Loss: 0.17063787579536438\n",
      "Training Loss: 0.17064779996871948\n",
      "Training Loss: 0.18020905554294586\n",
      "Training Loss: 0.1746709942817688\n",
      "Training Loss: 0.17145931720733643\n",
      "Training Loss: 0.15897336602210999\n",
      "Training Loss: 0.1396566778421402\n",
      "Training Loss: 0.186864972114563\n",
      "Training Loss: 0.17218469083309174\n",
      "Training Loss: 0.17416298389434814\n",
      "Training Loss: 0.18858084082603455\n",
      "Training Loss: 0.16305413842201233\n",
      "Training Loss: 0.18160010874271393\n",
      "Training Loss: 0.1809067577123642\n",
      "Training Loss: 0.1419975757598877\n",
      "Training Loss: 0.1626560539007187\n",
      "Training Loss: 0.17433111369609833\n",
      "Training Loss: 0.17773538827896118\n",
      "Training Loss: 0.17613455653190613\n",
      "Training Loss: 0.13231772184371948\n",
      "Training Loss: 0.14349788427352905\n",
      "Training Loss: 0.17787019908428192\n",
      "Training Loss: 0.17219063639640808\n",
      "Training Loss: 0.14876431226730347\n",
      "Training Loss: 0.17593075335025787\n",
      "Training Loss: 0.17124992609024048\n",
      "Training Loss: 0.13540352880954742\n",
      "Training Loss: 0.1591295748949051\n",
      "Training Loss: 0.1614188551902771\n",
      "Training Loss: 0.16893687844276428\n",
      "Training Loss: 0.1457706093788147\n",
      "Training Loss: 0.17134416103363037\n",
      "Training Loss: 0.14734689891338348\n",
      "Training Loss: 0.1673380583524704\n",
      "Training Loss: 0.1744905710220337\n",
      "Training Loss: 0.14884833991527557\n",
      "Training Loss: 0.16955174505710602\n",
      "Training Loss: 0.1386609673500061\n",
      "Training Loss: 0.12498411536216736\n",
      "Training Loss: 0.11947321891784668\n",
      "Training Loss: 0.15109877288341522\n",
      "Training Loss: 0.15085352957248688\n",
      "Training Loss: 0.15756265819072723\n",
      "Training Loss: 0.17681939899921417\n",
      "Training Loss: 0.18404534459114075\n",
      "Training Loss: 0.17481637001037598\n",
      "Training Loss: 0.18382245302200317\n",
      "Training Loss: 0.17720955610275269\n",
      "Training Loss: 0.18396620452404022\n",
      "Training Loss: 0.1459255963563919\n",
      "Training Loss: 0.14531199634075165\n",
      "Training Loss: 0.16644662618637085\n",
      "Training Loss: 0.17970703542232513\n",
      "Training Loss: 0.15002059936523438\n",
      "Training Loss: 0.17969486117362976\n",
      "Training Loss: 0.15151304006576538\n",
      "Training Loss: 0.17680324614048004\n",
      "Training Loss: 0.1488526314496994\n",
      "Training Loss: 0.14162243902683258\n",
      "Training Loss: 0.1494700312614441\n",
      "Training Loss: 0.13958978652954102\n",
      "Training Loss: 0.16539277136325836\n",
      "Training Loss: 0.15449203550815582\n",
      "Training Loss: 0.1421574205160141\n",
      "Training Loss: 0.14952923357486725\n",
      "Training Loss: 0.1696823090314865\n",
      "Training Loss: 0.16420559585094452\n",
      "Training Loss: 0.19203652441501617\n",
      "Training Loss: 0.16578947007656097\n",
      "Training Loss: 0.17263709008693695\n",
      "Training Loss: 0.14938081800937653\n",
      "Training Loss: 0.15145008265972137\n",
      "Training Loss: 0.16332164406776428\n",
      "Training Loss: 0.17572356760501862\n",
      "Training Loss: 0.14110924303531647\n",
      "Training Loss: 0.16254964470863342\n",
      "Training Loss: 0.1612684577703476\n",
      "Training Loss: 0.20753782987594604\n",
      "Training Loss: 0.1883087009191513\n",
      "Training Loss: 0.15513771772384644\n",
      "Training Loss: 0.18413962423801422\n",
      "Training Loss: 0.18449006974697113\n",
      "Training Loss: 0.1574312150478363\n",
      "Training Loss: 0.16100792586803436\n",
      "Training Loss: 0.18721872568130493\n",
      "Training Loss: 0.1911022663116455\n",
      "Training Loss: 0.1376170665025711\n",
      "Training Loss: 0.1886298507452011\n",
      "Training Loss: 0.17434877157211304\n",
      "Training Loss: 0.1819087415933609\n",
      "Training Loss: 0.16799207031726837\n",
      "Training Loss: 0.18797995150089264\n",
      "Training Loss: 0.1673823595046997\n",
      "Training Loss: 0.16135697066783905\n",
      "Training Loss: 0.18970587849617004\n",
      "Training Loss: 0.16383476555347443\n",
      "Training Loss: 0.17945748567581177\n",
      "Training Loss: 0.16227038204669952\n",
      "Training Loss: 0.15961430966854095\n",
      "Training Loss: 0.15905381739139557\n",
      "Training Loss: 0.18123944103717804\n",
      "Training Loss: 0.17800790071487427\n",
      "Training Loss: 0.17567111551761627\n",
      "Training Loss: 0.1512935310602188\n",
      "Training Loss: 0.1778547167778015\n",
      "Training Loss: 0.15153151750564575\n",
      "Training Loss: 0.17847490310668945\n",
      "Training Loss: 0.1872597634792328\n",
      "Training Loss: 0.17698204517364502\n",
      "Training Loss: 0.15501768887043\n",
      "Training Loss: 0.14316079020500183\n",
      "Training Loss: 0.18068566918373108\n",
      "Training Loss: 0.15620186924934387\n",
      "Training Loss: 0.14739860594272614\n",
      "Training Loss: 0.16575902700424194\n",
      "Training Loss: 0.15129834413528442\n",
      "Training Loss: 0.17470088601112366\n",
      "Training Loss: 0.1596117615699768\n",
      "Training Loss: 0.15210005640983582\n",
      "Training Loss: 0.1571701169013977\n",
      "Training Loss: 0.16409510374069214\n",
      "Training Loss: 0.17432650923728943\n",
      "Training Loss: 0.1385253220796585\n",
      "Training Loss: 0.14714394509792328\n",
      "Training Loss: 0.15755613148212433\n",
      "Training Loss: 0.13672752678394318\n",
      "Training Loss: 0.17034795880317688\n",
      "Training Loss: 0.1745472550392151\n",
      "Training Loss: 0.16806913912296295\n",
      "Training Loss: 0.16512960195541382\n",
      "Training Loss: 0.16514287889003754\n",
      "Training Loss: 0.17123088240623474\n",
      "Training Loss: 0.161875918507576\n",
      "Training Loss: 0.1556008905172348\n",
      "Training Loss: 0.16440778970718384\n",
      "Training Loss: 0.16500705480575562\n",
      "Training Loss: 0.12515254318714142\n",
      "Training Loss: 0.16755841672420502\n",
      "Training Loss: 0.1392725259065628\n",
      "Training Loss: 0.14042526483535767\n",
      "Training Loss: 0.16784262657165527\n",
      "Training Loss: 0.15707649290561676\n",
      "Training Loss: 0.1228296086192131\n",
      "Training Loss: 0.17382240295410156\n",
      "Training Loss: 0.18470829725265503\n",
      "Training Loss: 0.18367454409599304\n",
      "Training Loss: 0.1662713587284088\n",
      "Training Loss: 0.13741999864578247\n",
      "Training Loss: 0.16335004568099976\n",
      "Training Loss: 0.15453650057315826\n",
      "Training Loss: 0.15024584531784058\n",
      "Training Loss: 0.1815357506275177\n",
      "Training Loss: 0.14328111708164215\n",
      "Training Loss: 0.15363535284996033\n",
      "Training Loss: 0.1619013547897339\n",
      "Training Loss: 0.15903227031230927\n",
      "Training Loss: 0.17036160826683044\n",
      "Training Loss: 0.17544513940811157\n",
      "Training Loss: 0.17162734270095825\n",
      "Training Loss: 0.15928392112255096\n",
      "Training Loss: 0.14803913235664368\n",
      "Training Loss: 0.14067190885543823\n",
      "Training Loss: 0.16141147911548615\n",
      "Training Loss: 0.2137443572282791\n",
      "Training Loss: 0.19385242462158203\n",
      "Training Loss: 0.16970643401145935\n",
      "Training Loss: 0.1508902609348297\n",
      "Training Loss: 0.16466297209262848\n",
      "Training Loss: 0.15800949931144714\n",
      "Training Loss: 0.18014156818389893\n",
      "Training Loss: 0.15722835063934326\n",
      "Training Loss: 0.16207055747509003\n",
      "Training Loss: 0.188548281788826\n",
      "Training Loss: 0.1273804008960724\n",
      "Training Loss: 0.15930262207984924\n",
      "Training Loss: 0.17857244610786438\n",
      "Training Loss: 0.15689413249492645\n",
      "Training Loss: 0.14326229691505432\n",
      "Training Loss: 0.15989291667938232\n",
      "Training Loss: 0.18138152360916138\n",
      "Training Loss: 0.17265111207962036\n",
      "Training Loss: 0.17550235986709595\n",
      "Training Loss: 0.15589872002601624\n",
      "Training Loss: 0.1527917981147766\n",
      "Training Loss: 0.177822545170784\n",
      "Training Loss: 0.17806550860404968\n",
      "Training Loss: 0.18106217682361603\n",
      "Training Loss: 0.14479398727416992\n",
      "Training Loss: 0.1671248972415924\n",
      "Training Loss: 0.1574627012014389\n",
      "Training Loss: 0.14078623056411743\n",
      "Training Loss: 0.15793190896511078\n",
      "Training Loss: 0.14636856317520142\n",
      "Training Loss: 0.1776825189590454\n",
      "Training Loss: 0.1332910656929016\n",
      "Training Loss: 0.17811331152915955\n",
      "Training Loss: 0.17384476959705353\n",
      "Training Loss: 0.15054088830947876\n",
      "Training Loss: 0.13974328339099884\n",
      "Training Loss: 0.13883374631404877\n",
      "Training Loss: 0.14791111648082733\n",
      "Training Loss: 0.15305092930793762\n",
      "Training Loss: 0.1634698510169983\n",
      "Training Loss: 0.13357706367969513\n",
      "Training Loss: 0.16425538063049316\n",
      "Training Loss: 0.15817797183990479\n",
      "Training Loss: 0.13607968389987946\n",
      "Training Loss: 0.17303577065467834\n",
      "Training Loss: 0.1576775312423706\n",
      "Training Loss: 0.15260669589042664\n",
      "Training Loss: 0.16167202591896057\n",
      "Training Loss: 0.176539808511734\n",
      "Training Loss: 0.18106809258460999\n",
      "Training Loss: 0.16324873268604279\n",
      "Training Loss: 0.15311084687709808\n",
      "Training Loss: 0.16459594666957855\n",
      "Training Loss: 0.14863216876983643\n",
      "Training Loss: 0.152335062623024\n",
      "Training Loss: 0.13905803859233856\n",
      "Training Loss: 0.15523433685302734\n",
      "Training Loss: 0.1564418077468872\n",
      "Training Loss: 0.16843433678150177\n",
      "Training Loss: 0.16158106923103333\n",
      "Training Loss: 0.18558423221111298\n",
      "Training Loss: 0.15454967319965363\n",
      "Training Loss: 0.15611273050308228\n",
      "Training Loss: 0.1923869401216507\n",
      "Training Loss: 0.15172450244426727\n",
      "Training Loss: 0.16152769327163696\n",
      "Training Loss: 0.1543181836605072\n",
      "Training Loss: 0.1397942453622818\n",
      "Training Loss: 0.17621485888957977\n",
      "Training Loss: 0.15220226347446442\n",
      "Training Loss: 0.12834703922271729\n",
      "Training Loss: 0.16531860828399658\n",
      "Training Loss: 0.1721547245979309\n",
      "Training Loss: 0.14166828989982605\n",
      "Training Loss: 0.15122170746326447\n",
      "Training Loss: 0.14511853456497192\n",
      "Training Loss: 0.13446852564811707\n",
      "Training Loss: 0.18324191868305206\n",
      "Training Loss: 0.14071911573410034\n",
      "Training Loss: 0.1709917187690735\n",
      "Training Loss: 0.1823050081729889\n",
      "Training Loss: 0.16012054681777954\n",
      "Training Loss: 0.20141549408435822\n",
      "Training Loss: 0.18083272874355316\n",
      "Training Loss: 0.20627270638942719\n",
      "Training Loss: 0.1642957180738449\n",
      "Training Loss: 0.15955716371536255\n",
      "Training Loss: 0.16053032875061035\n",
      "Training Loss: 0.1405276209115982\n",
      "Training Loss: 0.16780321300029755\n",
      "Training Loss: 0.13697464764118195\n",
      "Training Loss: 0.17445126175880432\n",
      "Training Loss: 0.15900413691997528\n",
      "Training Loss: 0.1627020388841629\n",
      "Training Loss: 0.17702248692512512\n",
      "Training Loss: 0.15215446054935455\n",
      "Training Loss: 0.14987757802009583\n",
      "Training Loss: 0.17147092521190643\n",
      "Training Loss: 0.1526874452829361\n",
      "Training Loss: 0.17260879278182983\n",
      "Training Loss: 0.150128573179245\n",
      "Training Loss: 0.15739187598228455\n",
      "Training Loss: 0.18231593072414398\n",
      "Training Loss: 0.1479032337665558\n",
      "Training Loss: 0.15536683797836304\n",
      "Training Loss: 0.16147328913211823\n",
      "Training Loss: 0.16130448877811432\n",
      "Training Loss: 0.15988300740718842\n",
      "Training Loss: 0.13081969320774078\n",
      "Training Loss: 0.15243308246135712\n",
      "Training Loss: 0.17459890246391296\n",
      "Training Loss: 0.15798209607601166\n",
      "Training Loss: 0.14872866868972778\n",
      "Training Loss: 0.14110933244228363\n",
      "Training Loss: 0.1509326696395874\n",
      "Training Loss: 0.17993880808353424\n",
      "Training Loss: 0.1808328777551651\n",
      "Training Loss: 0.20098112523555756\n",
      "Training Loss: 0.1471461057662964\n",
      "Training Loss: 0.18824560940265656\n",
      "Training Loss: 0.1396154761314392\n",
      "Training Loss: 0.1574101746082306\n",
      "Training Loss: 0.18609102070331573\n",
      "Training Loss: 0.1564389020204544\n",
      "Training Loss: 0.17042328417301178\n",
      "Training Loss: 0.13893461227416992\n",
      "Training Loss: 0.12839484214782715\n",
      "Training Loss: 0.14437848329544067\n",
      "Training Loss: 0.16395920515060425\n",
      "Training Loss: 0.18670417368412018\n",
      "Training Loss: 0.15742626786231995\n",
      "Training Loss: 0.17614109814167023\n",
      "Training Loss: 0.1572599709033966\n",
      "Training Loss: 0.15403813123703003\n",
      "Training Loss: 0.16795353591442108\n",
      "Training Loss: 0.16633090376853943\n",
      "Training Loss: 0.15316271781921387\n",
      "Training Loss: 0.16714833676815033\n",
      "Training Loss: 0.18095719814300537\n",
      "Training Loss: 0.15706659853458405\n",
      "Training Loss: 0.161394864320755\n",
      "Training Loss: 0.17425113916397095\n",
      "Training Loss: 0.19343890249729156\n",
      "Training Loss: 0.18008427321910858\n",
      "Training Loss: 0.1566530168056488\n",
      "Training Loss: 0.15262222290039062\n",
      "Training Loss: 0.16525758802890778\n",
      "Training Loss: 0.1543024480342865\n",
      "Training Loss: 0.14939379692077637\n",
      "Training Loss: 0.16045048832893372\n",
      "Training Loss: 0.16435378789901733\n",
      "Training Loss: 0.17201487720012665\n",
      "Training Loss: 0.16439127922058105\n",
      "Training Loss: 0.16858181357383728\n",
      "Training Loss: 0.18453237414360046\n",
      "Training Loss: 0.15880095958709717\n",
      "Training Loss: 0.1551426649093628\n",
      "Training Loss: 0.1470457911491394\n",
      "Training Loss: 0.1585165560245514\n",
      "Training Loss: 0.1638508439064026\n",
      "Training Loss: 0.1659742295742035\n",
      "Training Loss: 0.13664668798446655\n",
      "Training Loss: 0.17109155654907227\n",
      "Training Loss: 0.13333889842033386\n",
      "Training Loss: 0.1544397622346878\n",
      "Training Loss: 0.16451698541641235\n",
      "Training Loss: 0.1474868655204773\n",
      "Training Loss: 0.15393801033496857\n",
      "Training Loss: 0.15404540300369263\n",
      "Training Loss: 0.17675064504146576\n",
      "Training Loss: 0.16698810458183289\n",
      "Training Loss: 0.14986200630664825\n",
      "Training Loss: 0.11943629384040833\n",
      "Training Loss: 0.13269440829753876\n",
      "Training Loss: 0.1486690640449524\n",
      "Training Loss: 0.15258115530014038\n",
      "Training Loss: 0.13421866297721863\n",
      "Training Loss: 0.13665589690208435\n",
      "Training Loss: 0.15919440984725952\n",
      "Training Loss: 0.1534029245376587\n",
      "Training Loss: 0.16269895434379578\n",
      "Training Loss: 0.16303472220897675\n",
      "Training Loss: 0.16367000341415405\n",
      "Training Loss: 0.14986753463745117\n",
      "Training Loss: 0.13830579817295074\n",
      "Training Loss: 0.15504492819309235\n",
      "Training Loss: 0.16664528846740723\n",
      "Training Loss: 0.15491026639938354\n",
      "Training Loss: 0.1654309779405594\n",
      "Training Loss: 0.17042984068393707\n",
      "Training Loss: 0.15103618800640106\n",
      "Training Loss: 0.18480810523033142\n",
      "Training Loss: 0.15026754140853882\n",
      "Training Loss: 0.14054271578788757\n",
      "Training Loss: 0.15379181504249573\n",
      "Training Loss: 0.16885149478912354\n",
      "Training Loss: 0.14791332185268402\n",
      "Training Loss: 0.18191130459308624\n",
      "Training Loss: 0.17058943212032318\n",
      "Training Loss: 0.13166330754756927\n",
      "Training Loss: 0.16849584877490997\n",
      "Training Loss: 0.1570395827293396\n",
      "Training Loss: 0.1785913109779358\n",
      "Training Loss: 0.13789494335651398\n",
      "Training Loss: 0.13299328088760376\n",
      "Training Loss: 0.1528151035308838\n",
      "Training Loss: 0.1487414687871933\n",
      "Training Loss: 0.14203183352947235\n",
      "Training Loss: 0.13268470764160156\n",
      "Training Loss: 0.1212448924779892\n",
      "Training Loss: 0.14609704911708832\n",
      "Training Loss: 0.1574588119983673\n",
      "Training Loss: 0.17800958454608917\n",
      "Training Loss: 0.1261894851922989\n",
      "Training Loss: 0.18362843990325928\n",
      "Training Loss: 0.15809005498886108\n",
      "Training Loss: 0.19969069957733154\n",
      "Training Loss: 0.15493744611740112\n",
      "Training Loss: 0.18990135192871094\n",
      "Training Loss: 0.16862426698207855\n",
      "Training Loss: 0.15723255276679993\n",
      "Training Loss: 0.16814923286437988\n",
      "Training Loss: 0.15484972298145294\n",
      "Training Loss: 0.15023690462112427\n",
      "Training Loss: 0.16312165558338165\n",
      "Training Loss: 0.16658729314804077\n",
      "Training Loss: 0.17706595361232758\n",
      "Training Loss: 0.18645982444286346\n",
      "Training Loss: 0.15191563963890076\n",
      "Training Loss: 0.13732707500457764\n",
      "Training Loss: 0.17021524906158447\n",
      "Training Loss: 0.14499734342098236\n",
      "Training Loss: 0.14001616835594177\n",
      "Training Loss: 0.1630869209766388\n",
      "Training Loss: 0.1463412046432495\n",
      "Training Loss: 0.1740596443414688\n",
      "Training Loss: 0.15192776918411255\n",
      "Training Loss: 0.1694774478673935\n",
      "Training Loss: 0.2015637904405594\n",
      "Training Loss: 0.18335022032260895\n",
      "Training Loss: 0.1888156235218048\n",
      "Training Loss: 0.17193211615085602\n",
      "Training Loss: 0.15590253472328186\n",
      "Training Loss: 0.16585449874401093\n",
      "Training Loss: 0.15448957681655884\n",
      "Training Loss: 0.15459011495113373\n",
      "Training Loss: 0.1596229076385498\n",
      "Training Loss: 0.16486172378063202\n",
      "Training Loss: 0.1876375824213028\n",
      "Training Loss: 0.18064051866531372\n",
      "Training Loss: 0.16172724962234497\n",
      "Training Loss: 0.16381727159023285\n",
      "Training Loss: 0.14309896528720856\n",
      "Training Loss: 0.15099725127220154\n",
      "Training Loss: 0.14244695007801056\n",
      "Training Loss: 0.17150180041790009\n",
      "Training Loss: 0.13617108762264252\n",
      "Training Loss: 0.16453684866428375\n",
      "Training Loss: 0.15906618535518646\n",
      "Training Loss: 0.13017016649246216\n",
      "Training Loss: 0.1527547836303711\n",
      "Training Loss: 0.1569060981273651\n",
      "Training Loss: 0.1524050086736679\n",
      "Training Loss: 0.16331413388252258\n",
      "Training Loss: 0.133283793926239\n",
      "Training Loss: 0.14978383481502533\n",
      "Training Loss: 0.20365478098392487\n",
      "Training Loss: 0.16841594874858856\n",
      "Training Loss: 0.1854363977909088\n",
      "Training Loss: 0.14106346666812897\n",
      "Training Loss: 0.15388768911361694\n",
      "Training Loss: 0.15990157425403595\n",
      "Training Loss: 0.1663135588169098\n",
      "Training Loss: 0.16932682693004608\n",
      "Training Loss: 0.17203909158706665\n",
      "Training Loss: 0.13813763856887817\n",
      "Training Loss: 0.15187256038188934\n",
      "Training Loss: 0.14243236184120178\n",
      "Training Loss: 0.12688788771629333\n",
      "Training Loss: 0.16773691773414612\n",
      "Training Loss: 0.18054169416427612\n",
      "Training Loss: 0.15740282833576202\n",
      "Training Loss: 0.16045135259628296\n",
      "Training Loss: 0.1642020046710968\n",
      "Training Loss: 0.18008102476596832\n",
      "Training Loss: 0.17202091217041016\n",
      "Training Loss: 0.15521006286144257\n",
      "Training Loss: 0.18156296014785767\n",
      "Training Loss: 0.14783143997192383\n",
      "Training Loss: 0.14669734239578247\n",
      "Training Loss: 0.16161035001277924\n",
      "Training Loss: 0.15763700008392334\n",
      "Training Loss: 0.14740493893623352\n",
      "Training Loss: 0.14276377856731415\n",
      "Training Loss: 0.16236726939678192\n",
      "Training Loss: 0.137785866856575\n",
      "Training Loss: 0.16728352010250092\n",
      "Training Loss: 0.1551796793937683\n",
      "Training Loss: 0.15423892438411713\n",
      "Training Loss: 0.1597432643175125\n",
      "Training Loss: 0.14223521947860718\n",
      "Training Loss: 0.1600923091173172\n",
      "Training Loss: 0.14853860437870026\n",
      "Training Loss: 0.16437454521656036\n",
      "Training Loss: 0.18837778270244598\n",
      "Training Loss: 0.16184157133102417\n",
      "Training Loss: 0.16710661351680756\n",
      "Training Loss: 0.17907480895519257\n",
      "Training Loss: 0.1536024510860443\n",
      "Training Loss: 0.1591305136680603\n",
      "Training Loss: 0.16195954382419586\n",
      "Training Loss: 0.16210497915744781\n",
      "Training Loss: 0.16500955820083618\n",
      "Training Loss: 0.17504075169563293\n",
      "Training Loss: 0.15692462027072906\n",
      "Training Loss: 0.1261804699897766\n",
      "Training Loss: 0.15807364881038666\n",
      "Training Loss: 0.15287409722805023\n",
      "Training Loss: 0.1426296830177307\n",
      "Training Loss: 0.14847880601882935\n",
      "Training Loss: 0.19841687381267548\n",
      "Training Loss: 0.1338634043931961\n",
      "Training Loss: 0.1416500210762024\n",
      "Training Loss: 0.1657993197441101\n",
      "Training Loss: 0.1294495314359665\n",
      "Training Loss: 0.17369550466537476\n",
      "Training Loss: 0.15789486467838287\n",
      "Training Loss: 0.14152967929840088\n",
      "Training Loss: 0.15967024862766266\n",
      "Training Loss: 0.15070390701293945\n",
      "Training Loss: 0.18239042162895203\n",
      "Training Loss: 0.16384880244731903\n",
      "Training Loss: 0.11045479774475098\n",
      "Training Loss: 0.1421695351600647\n",
      "Training Loss: 0.1583668738603592\n",
      "Training Loss: 0.15383167564868927\n",
      "Training Loss: 0.16506248712539673\n",
      "Training Loss: 0.1585163176059723\n",
      "Training Loss: 0.15516632795333862\n",
      "Training Loss: 0.18743720650672913\n",
      "Training Loss: 0.15176159143447876\n",
      "Training Loss: 0.17106693983078003\n",
      "Training Loss: 0.1420268714427948\n",
      "Training Loss: 0.11837533116340637\n",
      "Training Loss: 0.140625\n",
      "Training Loss: 0.17292864620685577\n",
      "Training Loss: 0.13668255507946014\n",
      "Training Loss: 0.17319291830062866\n",
      "Training Loss: 0.18686895072460175\n",
      "Training Loss: 0.14962197840213776\n",
      "Training Loss: 0.15851755440235138\n",
      "Training Loss: 0.147027850151062\n",
      "Training Loss: 0.14882762730121613\n",
      "Training Loss: 0.17122139036655426\n",
      "Training Loss: 0.17860935628414154\n",
      "Training Loss: 0.15730661153793335\n",
      "Training Loss: 0.17255176603794098\n",
      "Training Loss: 0.12056306004524231\n",
      "Training Loss: 0.1598692536354065\n",
      "Training Loss: 0.18342861533164978\n",
      "Training Loss: 0.15320102870464325\n",
      "Training Loss: 0.1618909388780594\n",
      "Training Loss: 0.16474661231040955\n",
      "Training Loss: 0.1723925620317459\n",
      "Training Loss: 0.1606122851371765\n",
      "Training Loss: 0.16125985980033875\n",
      "Training Loss: 0.15070529282093048\n",
      "Training Loss: 0.1446787565946579\n",
      "Training Loss: 0.1646166741847992\n",
      "Training Loss: 0.1304604560136795\n",
      "Training Loss: 0.1878281980752945\n",
      "Training Loss: 0.16328538954257965\n",
      "Training Loss: 0.1653347760438919\n",
      "Training Loss: 0.17593686282634735\n",
      "Training Loss: 0.16105499863624573\n",
      "Training Loss: 0.17725394666194916\n",
      "Training Loss: 0.17191481590270996\n",
      "Training Loss: 0.1733281910419464\n",
      "Training Loss: 0.15869535505771637\n",
      "Training Loss: 0.16235816478729248\n",
      "Training Loss: 0.16250060498714447\n",
      "Training Loss: 0.17383800446987152\n",
      "Training Loss: 0.1765783578157425\n",
      "Training Loss: 0.147232323884964\n",
      "Training Loss: 0.14741584658622742\n",
      "Training Loss: 0.11701498925685883\n",
      "Training Loss: 0.12974458932876587\n",
      "Training Loss: 0.18337255716323853\n",
      "Training Loss: 0.16118735074996948\n",
      "Training Loss: 0.17965181171894073\n",
      "Training Loss: 0.10613976418972015\n",
      "Training Loss: 0.1597704440355301\n",
      "Training Loss: 0.17562325298786163\n",
      "Training Loss: 0.1819514036178589\n",
      "Training Loss: 0.16296926140785217\n",
      "Training Loss: 0.1617087721824646\n",
      "Training Loss: 0.15577571094036102\n",
      "Training Loss: 0.1529894769191742\n",
      "Training Loss: 0.15399058163166046\n",
      "Training Loss: 0.13394661247730255\n",
      "Training Loss: 0.13616055250167847\n",
      "Training Loss: 0.14865177869796753\n",
      "Training Loss: 0.18050050735473633\n",
      "Training Loss: 0.1634407490491867\n",
      "Training Loss: 0.13887034356594086\n",
      "Training Loss: 0.1402968019247055\n",
      "Training Loss: 0.17712882161140442\n",
      "Training Loss: 0.16695095598697662\n",
      "Training Loss: 0.16187319159507751\n",
      "Training Loss: 0.16463522613048553\n",
      "Training Loss: 0.15089748799800873\n",
      "Training Loss: 0.14953617751598358\n",
      "Training Loss: 0.16128654778003693\n",
      "Training Loss: 0.1309937983751297\n",
      "Training Loss: 0.1551981419324875\n",
      "Training Loss: 0.1555047482252121\n",
      "Training Loss: 0.13086998462677002\n",
      "Training Loss: 0.16034714877605438\n",
      "Training Loss: 0.15821003913879395\n",
      "Training Loss: 0.1660849153995514\n",
      "Training Loss: 0.13825257122516632\n",
      "Training Loss: 0.14760687947273254\n",
      "Training Loss: 0.1581898182630539\n",
      "Training Loss: 0.10924062132835388\n",
      "Training Loss: 0.14971525967121124\n",
      "Training Loss: 0.14674687385559082\n",
      "Training Loss: 0.15665490925312042\n",
      "Training Loss: 0.13808248937129974\n",
      "Training Loss: 0.16384896636009216\n",
      "Training Loss: 0.20610898733139038\n",
      "Training Loss: 0.14014142751693726\n",
      "Training Loss: 0.1758376955986023\n",
      "Training Loss: 0.12955784797668457\n",
      "Training Loss: 0.1418313831090927\n",
      "Training Loss: 0.18434399366378784\n",
      "Training Loss: 0.13878384232521057\n",
      "Training Loss: 0.15737205743789673\n",
      "Training Loss: 0.17203450202941895\n",
      "Training Loss: 0.16664430499076843\n",
      "Training Loss: 0.1641690880060196\n",
      "Training Loss: 0.12893205881118774\n",
      "Training Loss: 0.1500900536775589\n",
      "Training Loss: 0.1437486857175827\n",
      "Training Loss: 0.10933002829551697\n",
      "Training Loss: 0.15887977182865143\n",
      "Training Loss: 0.14679402112960815\n",
      "Training Loss: 0.1702428162097931\n",
      "Training Loss: 0.16679948568344116\n",
      "Training Loss: 0.1591181457042694\n",
      "Training Loss: 0.14976276457309723\n",
      "Training Loss: 0.149543896317482\n",
      "Training Loss: 0.17435148358345032\n",
      "Training Loss: 0.16777728497982025\n",
      "Training Loss: 0.16761046648025513\n",
      "Training Loss: 0.16264744102954865\n",
      "Training Loss: 0.15376940369606018\n",
      "Training Loss: 0.15812335908412933\n",
      "Training Loss: 0.15548986196517944\n",
      "Training Loss: 0.16028207540512085\n",
      "Training Loss: 0.1339188516139984\n",
      "Training Loss: 0.166734516620636\n",
      "Training Loss: 0.1691243201494217\n",
      "Training Loss: 0.1369221806526184\n",
      "Training Loss: 0.15574206411838531\n",
      "Training Loss: 0.1500898152589798\n",
      "Training Loss: 0.1469772905111313\n",
      "Training Loss: 0.19436165690422058\n",
      "Training Loss: 0.13177992403507233\n",
      "Training Loss: 0.13284331560134888\n",
      "Training Loss: 0.14974814653396606\n",
      "Training Loss: 0.16494785249233246\n",
      "Training Loss: 0.177283376455307\n",
      "Training Loss: 0.16233225166797638\n",
      "Training Loss: 0.13062338531017303\n",
      "Training Loss: 0.16298677027225494\n",
      "Training Loss: 0.16575485467910767\n",
      "Training Loss: 0.14618073403835297\n",
      "Training Loss: 0.15626274049282074\n",
      "Training Loss: 0.16276487708091736\n",
      "Training Loss: 0.1830398589372635\n",
      "Training Loss: 0.14876462519168854\n",
      "Training Loss: 0.1487513780593872\n",
      "Training Loss: 0.17674103379249573\n",
      "Training Loss: 0.1724681556224823\n",
      "Training Loss: 0.14851170778274536\n",
      "Training Loss: 0.13675743341445923\n",
      "Training Loss: 0.17191538214683533\n",
      "Training Loss: 0.1285395324230194\n",
      "Training Loss: 0.153379425406456\n",
      "Training Loss: 0.12142297625541687\n",
      "Training Loss: 0.15684452652931213\n",
      "Training Loss: 0.20232044160366058\n",
      "Training Loss: 0.14276517927646637\n",
      "Training Loss: 0.15458747744560242\n",
      "Training Loss: 0.13790269196033478\n",
      "Training Loss: 0.15618516504764557\n",
      "Training Loss: 0.1816590577363968\n",
      "Training Loss: 0.13641344010829926\n",
      "Training Loss: 0.16126768290996552\n",
      "Training Loss: 0.1603228896856308\n",
      "Training Loss: 0.1593560129404068\n",
      "Training Loss: 0.11831823736429214\n",
      "Training Loss: 0.16375157237052917\n",
      "Training Loss: 0.16836515069007874\n",
      "Training Loss: 0.16587910056114197\n",
      "Training Loss: 0.12706947326660156\n",
      "Training Loss: 0.14613421261310577\n",
      "Training Loss: 0.14494384825229645\n",
      "Training Loss: 0.1325790286064148\n",
      "Training Loss: 0.15712827444076538\n",
      "Training Loss: 0.17205004394054413\n",
      "Training Loss: 0.1457725614309311\n",
      "Training Loss: 0.12505888938903809\n",
      "Training Loss: 0.14623647928237915\n",
      "Training Loss: 0.1384081244468689\n",
      "Training Loss: 0.16684818267822266\n",
      "Training Loss: 0.1469871699810028\n",
      "Training Loss: 0.1386445164680481\n",
      "Training Loss: 0.16288849711418152\n",
      "Training Loss: 0.17758725583553314\n",
      "Training Loss: 0.1475677639245987\n",
      "Training Loss: 0.14329732954502106\n",
      "Training Loss: 0.12516534328460693\n",
      "Training Loss: 0.13926854729652405\n",
      "Training Loss: 0.15613599121570587\n",
      "Training Loss: 0.1356552392244339\n",
      "Training Loss: 0.13184458017349243\n",
      "Training Loss: 0.1615266501903534\n",
      "Training Loss: 0.11913720518350601\n",
      "Training Loss: 0.18294335901737213\n",
      "Training Loss: 0.1577991545200348\n",
      "Training Loss: 0.13679634034633636\n",
      "Training Loss: 0.1554279774427414\n",
      "Training Loss: 0.12932388484477997\n",
      "Training Loss: 0.15748751163482666\n",
      "Training Loss: 0.1556290239095688\n",
      "Training Loss: 0.14457227289676666\n",
      "Training Loss: 0.14439888298511505\n",
      "Training Loss: 0.13176290690898895\n",
      "Training Loss: 0.14848464727401733\n",
      "Training Loss: 0.1558748185634613\n",
      "Training Loss: 0.1811855584383011\n",
      "Training Loss: 0.12315305322408676\n",
      "Training Loss: 0.1446118950843811\n",
      "Training Loss: 0.16019921004772186\n",
      "Training Loss: 0.16160687804222107\n",
      "Training Loss: 0.15255104005336761\n",
      "Training Loss: 0.1568729728460312\n",
      "Training Loss: 0.15476162731647491\n",
      "Training Loss: 0.1424277424812317\n",
      "Training Loss: 0.15729157626628876\n",
      "Training Loss: 0.14625416696071625\n",
      "Training Loss: 0.13442297279834747\n",
      "Training Loss: 0.1337365210056305\n",
      "Training Loss: 0.13562586903572083\n",
      "Training Loss: 0.1793709695339203\n",
      "Training Loss: 0.1546502560377121\n",
      "Training Loss: 0.14127936959266663\n",
      "Training Loss: 0.15843139588832855\n",
      "Training Loss: 0.1318420171737671\n",
      "Training Loss: 0.17182132601737976\n",
      "Training Loss: 0.17531351745128632\n",
      "Training Loss: 0.13600066304206848\n",
      "Training Loss: 0.14128859341144562\n",
      "Training Loss: 0.20354922115802765\n",
      "Training Loss: 0.19821277260780334\n",
      "Training Loss: 0.14885468780994415\n",
      "Training Loss: 0.14875932037830353\n",
      "Training Loss: 0.13589760661125183\n",
      "Training Loss: 0.16182108223438263\n",
      "Training Loss: 0.16680754721164703\n",
      "Training Loss: 0.15578439831733704\n",
      "Training Loss: 0.17023803293704987\n",
      "Training Loss: 0.13501042127609253\n",
      "Training Loss: 0.16669408977031708\n",
      "Training Loss: 0.13825875520706177\n",
      "Training Loss: 0.13725687563419342\n",
      "Training Loss: 0.15046606957912445\n",
      "Training Loss: 0.1449386477470398\n",
      "Training Loss: 0.1449640393257141\n",
      "Training Loss: 0.18374024331569672\n",
      "Training Loss: 0.1719263195991516\n",
      "Training Loss: 0.14517350494861603\n",
      "Training Loss: 0.13902658224105835\n",
      "Training Loss: 0.14725995063781738\n",
      "Training Loss: 0.1799382120370865\n",
      "Training Loss: 0.15047313272953033\n",
      "Training Loss: 0.16185015439987183\n",
      "Training Loss: 0.16567952930927277\n",
      "Training Loss: 0.1497815102338791\n",
      "Training Loss: 0.130540668964386\n",
      "Training Loss: 0.1291728913784027\n",
      "Training Loss: 0.14347973465919495\n",
      "Training Loss: 0.1177225410938263\n",
      "Training Loss: 0.15171974897384644\n",
      "Training Loss: 0.11986012011766434\n",
      "Training Loss: 0.17277902364730835\n",
      "Training Loss: 0.14745275676250458\n",
      "Training Loss: 0.1730281263589859\n",
      "Training Loss: 0.13208916783332825\n",
      "Training Loss: 0.16748544573783875\n",
      "Training Loss: 0.13995614647865295\n",
      "Training Loss: 0.15263248980045319\n",
      "Training Loss: 0.14486533403396606\n",
      "Training Loss: 0.1735169142484665\n",
      "Training Loss: 0.14433646202087402\n",
      "Training Loss: 0.16362448036670685\n",
      "Training Loss: 0.177604541182518\n",
      "Training Loss: 0.14738810062408447\n",
      "Training Loss: 0.13102367520332336\n",
      "Training Loss: 0.13828244805335999\n",
      "Training Loss: 0.16018399596214294\n",
      "Training Loss: 0.17384415864944458\n",
      "Training Loss: 0.1448994278907776\n",
      "Training Loss: 0.14942137897014618\n",
      "Training Loss: 0.1591286063194275\n",
      "Training Loss: 0.15857097506523132\n",
      "Training Loss: 0.155659019947052\n",
      "Training Loss: 0.15675824880599976\n",
      "Training Loss: 0.1292705237865448\n",
      "Training Loss: 0.14249669015407562\n",
      "Training Loss: 0.11568689346313477\n",
      "Training Loss: 0.1357634961605072\n",
      "Training Loss: 0.14362739026546478\n",
      "Training Loss: 0.18322938680648804\n",
      "Training Loss: 0.1482265293598175\n",
      "Training Loss: 0.15074066817760468\n",
      "Training Loss: 0.16109232604503632\n",
      "Training Loss: 0.13266772031784058\n",
      "Training Loss: 0.11470235884189606\n",
      "Training Loss: 0.13858620822429657\n",
      "Training Loss: 0.14632849395275116\n",
      "Training Loss: 0.1596786379814148\n",
      "Training Loss: 0.15034890174865723\n",
      "Training Loss: 0.13756337761878967\n",
      "Training Loss: 0.17269845306873322\n",
      "Training Loss: 0.1341446489095688\n",
      "Training Loss: 0.14092731475830078\n",
      "Training Loss: 0.1508987993001938\n",
      "Training Loss: 0.1203787624835968\n",
      "Training Loss: 0.14509466290473938\n",
      "Training Loss: 0.1333862990140915\n",
      "Training Loss: 0.1435345858335495\n",
      "Training Loss: 0.15113160014152527\n",
      "Training Loss: 0.15729472041130066\n",
      "Training Loss: 0.13480576872825623\n",
      "Training Loss: 0.13782505691051483\n",
      "Training Loss: 0.15778863430023193\n",
      "Training Loss: 0.17034360766410828\n",
      "Training Loss: 0.14262181520462036\n",
      "Training Loss: 0.18554317951202393\n",
      "Training Loss: 0.12482151389122009\n",
      "Training Loss: 0.15615543723106384\n",
      "Training Loss: 0.13047975301742554\n",
      "Training Loss: 0.16401726007461548\n",
      "Training Loss: 0.15277351438999176\n",
      "Training Loss: 0.10700685530900955\n",
      "Training Loss: 0.1543070375919342\n",
      "Training Loss: 0.16056936979293823\n",
      "Training Loss: 0.11529877781867981\n",
      "Training Loss: 0.1611095666885376\n",
      "Training Loss: 0.12713485956192017\n",
      "Training Loss: 0.15103575587272644\n",
      "Training Loss: 0.15764132142066956\n",
      "Training Loss: 0.15094828605651855\n",
      "Training Loss: 0.14413708448410034\n",
      "Training Loss: 0.135074183344841\n",
      "Training Loss: 0.16293396055698395\n",
      "Training Loss: 0.1532723754644394\n",
      "Training Loss: 0.15644854307174683\n",
      "Training Loss: 0.14105349779129028\n",
      "Training Loss: 0.14503954350948334\n",
      "Training Loss: 0.16766105592250824\n",
      "Training Loss: 0.13850601017475128\n",
      "Training Loss: 0.14499370753765106\n",
      "Training Loss: 0.1898878514766693\n",
      "Training Loss: 0.14657558500766754\n",
      "Training Loss: 0.12462406605482101\n",
      "Training Loss: 0.13717186450958252\n",
      "Training Loss: 0.163974791765213\n",
      "Training Loss: 0.17362838983535767\n",
      "Training Loss: 0.17292290925979614\n",
      "Training Loss: 0.12977012991905212\n",
      "Training Loss: 0.15125861763954163\n",
      "Training Loss: 0.12206942588090897\n",
      "Training Loss: 0.15383422374725342\n",
      "Training Loss: 0.12728260457515717\n",
      "Training Loss: 0.15765544772148132\n",
      "Training Loss: 0.14137417078018188\n",
      "Training Loss: 0.15297770500183105\n",
      "Training Loss: 0.16354742646217346\n",
      "Training Loss: 0.13178770244121552\n",
      "Training Loss: 0.14542438089847565\n",
      "Training Loss: 0.14257746934890747\n",
      "Training Loss: 0.15280753374099731\n",
      "Training Loss: 0.17252217233181\n",
      "Training Loss: 0.11838564276695251\n",
      "Training Loss: 0.15915653109550476\n",
      "Training Loss: 0.1457623839378357\n",
      "Training Loss: 0.11034656316041946\n",
      "Training Loss: 0.11028275638818741\n",
      "Training Loss: 0.138885036110878\n",
      "Training Loss: 0.1491677165031433\n",
      "Training Loss: 0.1443650722503662\n",
      "Training Loss: 0.1407434046268463\n",
      "Training Loss: 0.1399814933538437\n",
      "Training Loss: 0.1603926420211792\n",
      "Training Loss: 0.1389828473329544\n",
      "Training Loss: 0.13939383625984192\n",
      "Training Loss: 0.14858640730381012\n",
      "Training Loss: 0.16204234957695007\n",
      "Training Loss: 0.1447620540857315\n",
      "Training Loss: 0.13716310262680054\n",
      "Training Loss: 0.12046493589878082\n",
      "Training Loss: 0.1377614587545395\n",
      "Training Loss: 0.14049199223518372\n",
      "Training Loss: 0.131033256649971\n",
      "Training Loss: 0.1529456079006195\n",
      "Training Loss: 0.16933615505695343\n",
      "Training Loss: 0.15124550461769104\n",
      "Training Loss: 0.1796678751707077\n",
      "Training Loss: 0.13725633919239044\n",
      "Training Loss: 0.12351073324680328\n",
      "Training Loss: 0.13367557525634766\n",
      "Training Loss: 0.13066095113754272\n",
      "Training Loss: 0.12402553111314774\n",
      "Training Loss: 0.137617290019989\n",
      "Training Loss: 0.13028231263160706\n",
      "Training Loss: 0.13363002240657806\n",
      "Training Loss: 0.1565738469362259\n",
      "Training Loss: 0.12387078255414963\n",
      "Training Loss: 0.10177977383136749\n",
      "Training Loss: 0.15104809403419495\n",
      "Training Loss: 0.13084226846694946\n",
      "Training Loss: 0.15393653512001038\n",
      "Training Loss: 0.13269560039043427\n",
      "Training Loss: 0.13498227298259735\n",
      "Training Loss: 0.11136172711849213\n",
      "Training Loss: 0.12005886435508728\n",
      "Training Loss: 0.1243751049041748\n",
      "Training Loss: 0.15062958002090454\n",
      "Training Loss: 0.16168178617954254\n",
      "Training Loss: 0.1412590593099594\n",
      "Training Loss: 0.12805107235908508\n",
      "Training Loss: 0.15611620247364044\n",
      "Training Loss: 0.14019379019737244\n",
      "Training Loss: 0.14850598573684692\n",
      "Training Loss: 0.1369561403989792\n",
      "Training Loss: 0.12213495373725891\n",
      "Training Loss: 0.11115828156471252\n",
      "Training Loss: 0.1343957781791687\n",
      "Training Loss: 0.13390249013900757\n",
      "Training Loss: 0.13687917590141296\n",
      "Training Loss: 0.14543214440345764\n",
      "Training Loss: 0.13415254652500153\n",
      "Training Loss: 0.13883422315120697\n",
      "Training Loss: 0.16320139169692993\n",
      "Training Loss: 0.14871329069137573\n",
      "Training Loss: 0.13895127177238464\n",
      "Training Loss: 0.13875220715999603\n",
      "Training Loss: 0.15827301144599915\n",
      "Training Loss: 0.14175273478031158\n",
      "Training Loss: 0.13572347164154053\n",
      "Training Loss: 0.10006443411111832\n",
      "Training Loss: 0.13998326659202576\n",
      "Training Loss: 0.13355235755443573\n",
      "Training Loss: 0.13564752042293549\n",
      "Training Loss: 0.14834067225456238\n",
      "Training Loss: 0.14980155229568481\n",
      "Training Loss: 0.1271234005689621\n",
      "Training Loss: 0.14560583233833313\n",
      "Training Loss: 0.14226754009723663\n",
      "Training Loss: 0.13863414525985718\n",
      "Training Loss: 0.16605089604854584\n",
      "Training Loss: 0.14547598361968994\n",
      "Training Loss: 0.1463213562965393\n",
      "Training Loss: 0.14418385922908783\n",
      "Training Loss: 0.14080911874771118\n",
      "Training Loss: 0.13914300501346588\n",
      "Training Loss: 0.12720151245594025\n",
      "Training Loss: 0.17023202776908875\n",
      "Training Loss: 0.11186549067497253\n",
      "Training Loss: 0.1372677981853485\n",
      "Training Loss: 0.10881654918193817\n",
      "Training Loss: 0.11917822808027267\n",
      "Training Loss: 0.13365812599658966\n",
      "Training Loss: 0.14604777097702026\n",
      "Training Loss: 0.15982966125011444\n",
      "Training Loss: 0.15062588453292847\n",
      "Training Loss: 0.15548735857009888\n",
      "Training Loss: 0.1368018090724945\n",
      "Training Loss: 0.14113125205039978\n",
      "Training Loss: 0.1611023247241974\n",
      "Training Loss: 0.1388363093137741\n",
      "Training Loss: 0.12395438551902771\n",
      "Training Loss: 0.1268601417541504\n",
      "Training Loss: 0.12444599717855453\n",
      "Training Loss: 0.15546858310699463\n",
      "Training Loss: 0.11473146080970764\n",
      "Training Loss: 0.1785861849784851\n",
      "Training Loss: 0.13482049107551575\n",
      "Training Loss: 0.12389300763607025\n",
      "Training Loss: 0.12012897431850433\n",
      "Training Loss: 0.13883137702941895\n",
      "Training Loss: 0.11994966864585876\n",
      "Training Loss: 0.13331985473632812\n",
      "Training Loss: 0.12861192226409912\n",
      "Training Loss: 0.13402079045772552\n",
      "Training Loss: 0.10754895210266113\n",
      "Training Loss: 0.1361929029226303\n",
      "Training Loss: 0.17354077100753784\n",
      "Training Loss: 0.12699057161808014\n",
      "Training Loss: 0.11722055077552795\n",
      "Training Loss: 0.13147467374801636\n",
      "Training Loss: 0.12255481630563736\n",
      "Training Loss: 0.1617242991924286\n",
      "Training Loss: 0.15757718682289124\n",
      "Training Loss: 0.11233331263065338\n",
      "Training Loss: 0.1460551619529724\n",
      "Training Loss: 0.13344615697860718\n",
      "Training Loss: 0.16712349653244019\n",
      "Training Loss: 0.1398797184228897\n",
      "Training Loss: 0.1270979791879654\n",
      "Training Loss: 0.13069413602352142\n",
      "Training Loss: 0.13255029916763306\n",
      "Training Loss: 0.11893419176340103\n",
      "Training Loss: 0.12870386242866516\n",
      "Training Loss: 0.09946872293949127\n",
      "Training Loss: 0.12061616778373718\n",
      "Training Loss: 0.1356157511472702\n",
      "Training Loss: 0.1078510582447052\n",
      "Training Loss: 0.11543635278940201\n",
      "Training Loss: 0.15096649527549744\n",
      "Training Loss: 0.15281039476394653\n",
      "Training Loss: 0.1344236582517624\n",
      "Training Loss: 0.14978262782096863\n",
      "Training Loss: 0.12527614831924438\n",
      "Training Loss: 0.11285075545310974\n",
      "Training Loss: 0.1328563690185547\n",
      "Training Loss: 0.135380357503891\n",
      "Training Loss: 0.13468456268310547\n",
      "Training Loss: 0.14353036880493164\n",
      "Training Loss: 0.15232683718204498\n",
      "Training Loss: 0.12307319045066833\n",
      "Training Loss: 0.10996641963720322\n",
      "Training Loss: 0.16860969364643097\n",
      "Training Loss: 0.14172020554542542\n",
      "Training Loss: 0.11075028777122498\n",
      "Training Loss: 0.1062081977725029\n",
      "Training Loss: 0.14144478738307953\n",
      "Training Loss: 0.12943698465824127\n",
      "Training Loss: 0.13917487859725952\n",
      "Training Loss: 0.12408418208360672\n",
      "Training Loss: 0.12950390577316284\n",
      "Training Loss: 0.11588028818368912\n",
      "Training Loss: 0.1333729773759842\n",
      "Training Loss: 0.13604973256587982\n",
      "Training Loss: 0.09941929578781128\n",
      "Training Loss: 0.12953051924705505\n",
      "Training Loss: 0.13337409496307373\n",
      "Training Loss: 0.1476244032382965\n",
      "Training Loss: 0.13750673830509186\n",
      "Training Loss: 0.1402953863143921\n",
      "Training Loss: 0.12555667757987976\n",
      "Training Loss: 0.14868144690990448\n",
      "Training Loss: 0.16860856115818024\n",
      "Training Loss: 0.12278416752815247\n",
      "Training Loss: 0.14757412672042847\n",
      "Training Loss: 0.1596352905035019\n",
      "Training Loss: 0.10675003379583359\n",
      "Training Loss: 0.1293744295835495\n",
      "Training Loss: 0.1336892694234848\n",
      "Training Loss: 0.16466012597084045\n",
      "Training Loss: 0.13687942922115326\n",
      "Training Loss: 0.14131243526935577\n",
      "Training Loss: 0.13245230913162231\n",
      "Training Loss: 0.11940731108188629\n",
      "Training Loss: 0.13955086469650269\n",
      "Training Loss: 0.11596545577049255\n",
      "Training Loss: 0.15567345917224884\n",
      "Training Loss: 0.1393296867609024\n",
      "Training Loss: 0.14383825659751892\n",
      "Training Loss: 0.1168404296040535\n",
      "Training Loss: 0.12256649881601334\n",
      "Training Loss: 0.1228996142745018\n",
      "Training Loss: 0.12977658212184906\n",
      "Training Loss: 0.08855771273374557\n",
      "Training Loss: 0.18053145706653595\n",
      "Training Loss: 0.12153095752000809\n",
      "Training Loss: 0.15409626066684723\n",
      "Training Loss: 0.10397922247648239\n",
      "Training Loss: 0.10181727260351181\n",
      "Training Loss: 0.1423860639333725\n",
      "Training Loss: 0.13236089050769806\n",
      "Training Loss: 0.1709185242652893\n",
      "Training Loss: 0.11886419355869293\n",
      "Training Loss: 0.13682930171489716\n",
      "Training Loss: 0.13552719354629517\n",
      "Training Loss: 0.13691115379333496\n",
      "Training Loss: 0.128420889377594\n",
      "Training Loss: 0.14157819747924805\n",
      "Training Loss: 0.1420198678970337\n",
      "Training Loss: 0.13132047653198242\n",
      "Training Loss: 0.12074808031320572\n",
      "Training Loss: 0.11639874428510666\n",
      "Training Loss: 0.11503950506448746\n",
      "Training Loss: 0.0947718396782875\n",
      "Training Loss: 0.1425045132637024\n",
      "Training Loss: 0.09608695656061172\n",
      "Training Loss: 0.11324617266654968\n",
      "Training Loss: 0.1255810409784317\n",
      "Training Loss: 0.11053845286369324\n",
      "Training Loss: 0.10150286555290222\n",
      "Training Loss: 0.11857827752828598\n",
      "Training Loss: 0.14399725198745728\n",
      "Training Loss: 0.1544061303138733\n",
      "Training Loss: 0.10399947315454483\n",
      "Training Loss: 0.1454419195652008\n",
      "Training Loss: 0.11554431915283203\n",
      "Training Loss: 0.12007194012403488\n",
      "Training Loss: 0.12027908861637115\n",
      "Training Loss: 0.13509614765644073\n",
      "Training Loss: 0.13725651800632477\n",
      "Training Loss: 0.14124451577663422\n",
      "Training Loss: 0.12376368045806885\n",
      "Training Loss: 0.14805550873279572\n",
      "Training Loss: 0.10283412039279938\n",
      "Training Loss: 0.12725108861923218\n",
      "Training Loss: 0.08451767265796661\n",
      "Training Loss: 0.11289094388484955\n",
      "Training Loss: 0.10529982298612595\n",
      "Training Loss: 0.12218549847602844\n",
      "Training Loss: 0.11357977986335754\n",
      "Training Loss: 0.15419305860996246\n",
      "Training Loss: 0.1265590786933899\n",
      "Training Loss: 0.10322649031877518\n",
      "Training Loss: 0.12449504435062408\n",
      "Training Loss: 0.15323974192142487\n",
      "Training Loss: 0.10617730766534805\n",
      "Training Loss: 0.13697616755962372\n",
      "Training Loss: 0.15642544627189636\n",
      "Training Loss: 0.1291859745979309\n",
      "Training Loss: 0.13002507388591766\n",
      "Training Loss: 0.1137172132730484\n",
      "Training Loss: 0.1532243937253952\n",
      "Training Loss: 0.11059369146823883\n",
      "Training Loss: 0.13804073631763458\n",
      "Training Loss: 0.14547309279441833\n",
      "Training Loss: 0.11079660058021545\n",
      "Training Loss: 0.1229136511683464\n",
      "Training Loss: 0.12812942266464233\n",
      "Training Loss: 0.14702054858207703\n",
      "Training Loss: 0.12210028618574142\n",
      "Training Loss: 0.13036519289016724\n",
      "Training Loss: 0.11226805299520493\n",
      "Training Loss: 0.12467159330844879\n",
      "Training Loss: 0.10264813154935837\n",
      "Training Loss: 0.14291314780712128\n",
      "Training Loss: 0.14187607169151306\n",
      "Training Loss: 0.1226639375090599\n",
      "Training Loss: 0.13768529891967773\n",
      "Training Loss: 0.11872896552085876\n",
      "Training Loss: 0.11760612577199936\n",
      "Training Loss: 0.09785906970500946\n",
      "Training Loss: 0.11517344415187836\n",
      "Training Loss: 0.16678841412067413\n",
      "Training Loss: 0.12170317769050598\n",
      "Training Loss: 0.11861301958560944\n",
      "Training Loss: 0.1165718361735344\n",
      "Training Loss: 0.10295004397630692\n",
      "Training Loss: 0.12910407781600952\n",
      "Training Loss: 0.16268929839134216\n",
      "Training Loss: 0.11602796614170074\n",
      "Training Loss: 0.11273075640201569\n",
      "Training Loss: 0.11681241542100906\n",
      "Training Loss: 0.15315638482570648\n",
      "Training Loss: 0.10732236504554749\n",
      "Training Loss: 0.1011071503162384\n",
      "Training Loss: 0.12166891992092133\n",
      "Training Loss: 0.14090481400489807\n",
      "Training Loss: 0.11820607632398605\n",
      "Training Loss: 0.1388980597257614\n",
      "Training Loss: 0.11367464065551758\n",
      "Training Loss: 0.12305153906345367\n",
      "Training Loss: 0.13923127949237823\n",
      "Training Loss: 0.12096340209245682\n",
      "Training Loss: 0.16091543436050415\n",
      "Training Loss: 0.10119691491127014\n",
      "Training Loss: 0.12031351029872894\n",
      "Training Loss: 0.12557637691497803\n",
      "Training Loss: 0.134750097990036\n",
      "Training Loss: 0.1263912171125412\n",
      "Training Loss: 0.15125638246536255\n",
      "Training Loss: 0.09904967993497849\n",
      "Training Loss: 0.11160758882761002\n",
      "Training Loss: 0.12850116193294525\n",
      "Training Loss: 0.12984655797481537\n",
      "Training Loss: 0.13645294308662415\n",
      "Training Loss: 0.11190075427293777\n",
      "Training Loss: 0.13758763670921326\n",
      "Training Loss: 0.09921936690807343\n",
      "Training Loss: 0.09288089722394943\n",
      "Training Loss: 0.1185503900051117\n",
      "Training Loss: 0.11033408343791962\n",
      "Training Loss: 0.11634968221187592\n",
      "Training Loss: 0.12637625634670258\n",
      "Training Loss: 0.10975248366594315\n",
      "Training Loss: 0.1513497233390808\n",
      "Training Loss: 0.15442593395709991\n",
      "Training Loss: 0.12191266566514969\n",
      "Training Loss: 0.10556583106517792\n",
      "Training Loss: 0.12214212864637375\n",
      "Training Loss: 0.09711551666259766\n",
      "Training Loss: 0.14133097231388092\n",
      "Training Loss: 0.10440472513437271\n",
      "Training Loss: 0.12626200914382935\n",
      "Training Loss: 0.12657858431339264\n",
      "Training Loss: 0.12910592555999756\n",
      "Training Loss: 0.10845133662223816\n",
      "Training Loss: 0.16861139237880707\n",
      "Training Loss: 0.11307772994041443\n",
      "Training Loss: 0.10606089234352112\n",
      "Training Loss: 0.1304163932800293\n",
      "Training Loss: 0.10265519469976425\n",
      "Training Loss: 0.14150021970272064\n",
      "Training Loss: 0.1356722116470337\n",
      "Training Loss: 0.11395839601755142\n",
      "Training Loss: 0.11415687203407288\n",
      "Training Loss: 0.11410275101661682\n",
      "Training Loss: 0.10823683440685272\n",
      "Training Loss: 0.10679680854082108\n",
      "Training Loss: 0.10143420100212097\n",
      "Training Loss: 0.11078295856714249\n",
      "Training Loss: 0.10864816606044769\n",
      "Training Loss: 0.11192148178815842\n",
      "Training Loss: 0.12936359643936157\n",
      "Training Loss: 0.0954512357711792\n",
      "Training Loss: 0.1034182757139206\n",
      "Training Loss: 0.1103237122297287\n",
      "Training Loss: 0.1325015127658844\n",
      "Training Loss: 0.11961343884468079\n",
      "Training Loss: 0.12999999523162842\n",
      "Training Loss: 0.12040780484676361\n",
      "Training Loss: 0.1132076159119606\n",
      "Training Loss: 0.10233746469020844\n",
      "Training Loss: 0.12427905201911926\n",
      "Training Loss: 0.10463083535432816\n",
      "Training Loss: 0.13960446417331696\n",
      "Training Loss: 0.11868829280138016\n",
      "Training Loss: 0.10992301255464554\n",
      "Training Loss: 0.11368885636329651\n",
      "Training Loss: 0.09485147893428802\n",
      "Training Loss: 0.12684719264507294\n",
      "Training Loss: 0.09205550700426102\n",
      "Training Loss: 0.1096700057387352\n",
      "Training Loss: 0.11707094311714172\n",
      "Training Loss: 0.08747556805610657\n",
      "Training Loss: 0.12350605428218842\n",
      "Training Loss: 0.10960057377815247\n",
      "Training Loss: 0.08914870768785477\n",
      "Training Loss: 0.09966340661048889\n",
      "Training Loss: 0.13360260426998138\n",
      "Training Loss: 0.09388645738363266\n",
      "Training Loss: 0.11741748452186584\n",
      "Training Loss: 0.12720347940921783\n",
      "Training Loss: 0.11749741435050964\n",
      "Training Loss: 0.10287657380104065\n",
      "Training Loss: 0.11639498174190521\n",
      "Training Loss: 0.11979476362466812\n",
      "Training Loss: 0.14000505208969116\n",
      "Training Loss: 0.0776429995894432\n",
      "Training Loss: 0.10407102108001709\n",
      "Training Loss: 0.08507401496171951\n",
      "Training Loss: 0.12163705378770828\n",
      "Training Loss: 0.14784415066242218\n",
      "Training Loss: 0.1326797604560852\n",
      "Training Loss: 0.12450562417507172\n",
      "Training Loss: 0.09757686406373978\n",
      "Training Loss: 0.12262209504842758\n",
      "Training Loss: 0.12839624285697937\n",
      "Training Loss: 0.12669990956783295\n",
      "Training Loss: 0.12476105242967606\n",
      "Training Loss: 0.1343212127685547\n",
      "Training Loss: 0.10581289976835251\n",
      "Training Loss: 0.10966566205024719\n",
      "Training Loss: 0.1045127734541893\n",
      "Training Loss: 0.11656560003757477\n",
      "Training Loss: 0.10037646442651749\n",
      "Training Loss: 0.09622766822576523\n",
      "Training Loss: 0.14087234437465668\n",
      "Training Loss: 0.1295575648546219\n",
      "Training Loss: 0.11995667964220047\n",
      "Training Loss: 0.10257098823785782\n",
      "Training Loss: 0.10376353561878204\n",
      "Training Loss: 0.1516689509153366\n",
      "Training Loss: 0.09379838407039642\n",
      "Training Loss: 0.13140437006950378\n",
      "Training Loss: 0.13013814389705658\n",
      "Training Loss: 0.10562874376773834\n",
      "Training Loss: 0.12303365767002106\n",
      "Training Loss: 0.11113391816616058\n",
      "Training Loss: 0.11287421733140945\n",
      "Training Loss: 0.13476607203483582\n",
      "Training Loss: 0.08564988523721695\n",
      "Training Loss: 0.13644187152385712\n",
      "Training Loss: 0.0946575179696083\n",
      "Training Loss: 0.09621933102607727\n",
      "Training Loss: 0.11199891567230225\n",
      "Training Loss: 0.10683955252170563\n",
      "Training Loss: 0.08114559203386307\n",
      "Training Loss: 0.11399628221988678\n",
      "Training Loss: 0.10104261338710785\n",
      "Training Loss: 0.13687732815742493\n",
      "Training Loss: 0.10063695162534714\n",
      "Training Loss: 0.1183663085103035\n",
      "Training Loss: 0.11737235635519028\n",
      "Training Loss: 0.08330914378166199\n",
      "Training Loss: 0.10101767629384995\n",
      "Training Loss: 0.11379305273294449\n",
      "Training Loss: 0.11861734837293625\n",
      "Training Loss: 0.10441441088914871\n",
      "Training Loss: 0.12226609140634537\n",
      "Training Loss: 0.12814179062843323\n",
      "Training Loss: 0.10730888694524765\n",
      "Training Loss: 0.131891131401062\n",
      "Training Loss: 0.1202913373708725\n",
      "Training Loss: 0.11592093110084534\n",
      "Training Loss: 0.10266803205013275\n",
      "Training Loss: 0.13523556292057037\n",
      "Training Loss: 0.11340214312076569\n",
      "Training Loss: 0.11428184062242508\n",
      "Training Loss: 0.11850014328956604\n",
      "Training Loss: 0.10129936784505844\n",
      "Training Loss: 0.11671704798936844\n",
      "Training Loss: 0.11072985082864761\n",
      "Training Loss: 0.10556412488222122\n",
      "Training Loss: 0.11297652125358582\n",
      "Training Loss: 0.09527380019426346\n",
      "Training Loss: 0.13833798468112946\n",
      "Training Loss: 0.08646035939455032\n",
      "Training Loss: 0.08935941010713577\n",
      "Training Loss: 0.1198902502655983\n",
      "Training Loss: 0.12137921899557114\n",
      "Training Loss: 0.09003278613090515\n",
      "Training Loss: 0.10725128650665283\n",
      "Training Loss: 0.12592917680740356\n",
      "Training Loss: 0.11201056838035583\n",
      "Training Loss: 0.10175488144159317\n",
      "Training Loss: 0.09486870467662811\n",
      "Training Loss: 0.10436539351940155\n",
      "Training Loss: 0.1186014711856842\n",
      "Training Loss: 0.11801329255104065\n",
      "Training Loss: 0.06901136040687561\n",
      "Training Loss: 0.09086302667856216\n",
      "Training Loss: 0.10437654703855515\n",
      "Training Loss: 0.15205423533916473\n",
      "Training Loss: 0.08662620186805725\n",
      "Training Loss: 0.08503589034080505\n",
      "Training Loss: 0.09778394550085068\n",
      "Training Loss: 0.1142459437251091\n",
      "Training Loss: 0.1497366577386856\n",
      "Training Loss: 0.12112835794687271\n",
      "Training Loss: 0.1378539353609085\n",
      "Training Loss: 0.12701033055782318\n",
      "Training Loss: 0.09848321974277496\n",
      "Training Loss: 0.10187490284442902\n",
      "Training Loss: 0.12774872779846191\n",
      "Training Loss: 0.10277274250984192\n",
      "Training Loss: 0.10097365081310272\n",
      "Training Loss: 0.09330704808235168\n",
      "Training Loss: 0.13136836886405945\n",
      "Training Loss: 0.10746952146291733\n",
      "Training Loss: 0.09404907375574112\n",
      "Training Loss: 0.07964256405830383\n",
      "Training Loss: 0.11882574111223221\n",
      "Training Loss: 0.12192560732364655\n",
      "Training Loss: 0.11147996783256531\n",
      "Training Loss: 0.08267626166343689\n",
      "Training Loss: 0.08888732641935349\n",
      "Training Loss: 0.13219885528087616\n",
      "Training Loss: 0.12898217141628265\n",
      "Training Loss: 0.08103390038013458\n",
      "Training Loss: 0.09386132657527924\n",
      "Training Loss: 0.11966851353645325\n",
      "Training Loss: 0.10496555268764496\n",
      "Training Loss: 0.12989994883537292\n",
      "Training Loss: 0.10807710140943527\n",
      "Training Loss: 0.0899820327758789\n",
      "Training Loss: 0.08934555947780609\n",
      "Training Loss: 0.07946913689374924\n",
      "Training Loss: 0.11123886704444885\n",
      "Training Loss: 0.12540896236896515\n",
      "Training Loss: 0.09380033612251282\n",
      "Training Loss: 0.1096154972910881\n",
      "Training Loss: 0.08086737990379333\n",
      "Training Loss: 0.082584448158741\n",
      "Training Loss: 0.09916399419307709\n",
      "Training Loss: 0.10769765824079514\n",
      "Training Loss: 0.10005875676870346\n",
      "Training Loss: 0.10171061754226685\n",
      "Training Loss: 0.10998877882957458\n",
      "Training Loss: 0.1293720006942749\n",
      "Training Loss: 0.08570918440818787\n",
      "Training Loss: 0.10458969324827194\n",
      "Training Loss: 0.0810556635260582\n",
      "Training Loss: 0.12642018496990204\n",
      "Training Loss: 0.09392106533050537\n",
      "Training Loss: 0.11106768995523453\n",
      "Training Loss: 0.11695981025695801\n",
      "Training Loss: 0.08351448178291321\n",
      "Training Loss: 0.0927937924861908\n",
      "Training Loss: 0.10862281173467636\n",
      "Training Loss: 0.11481253057718277\n",
      "Training Loss: 0.10162393748760223\n",
      "Training Loss: 0.09855068475008011\n",
      "Training Loss: 0.07288634777069092\n",
      "Training Loss: 0.0797029435634613\n",
      "Training Loss: 0.10304035991430283\n",
      "Training Loss: 0.12263619899749756\n",
      "Training Loss: 0.09452128410339355\n",
      "Training Loss: 0.09620080143213272\n",
      "Training Loss: 0.0952138751745224\n",
      "Training Loss: 0.10199450701475143\n",
      "Training Loss: 0.0863342434167862\n",
      "Training Loss: 0.08122365921735764\n",
      "Training Loss: 0.07937906682491302\n",
      "Training Loss: 0.10569635033607483\n",
      "Training Loss: 0.07998369634151459\n",
      "Training Loss: 0.1258697211742401\n",
      "Training Loss: 0.09557754546403885\n",
      "Training Loss: 0.11409658938646317\n",
      "Training Loss: 0.1274167001247406\n",
      "Training Loss: 0.10020409524440765\n",
      "Training Loss: 0.11286841332912445\n",
      "Training Loss: 0.1088181659579277\n",
      "Training Loss: 0.11477620154619217\n",
      "Training Loss: 0.10260682553052902\n",
      "Training Loss: 0.08333169668912888\n",
      "Training Loss: 0.07564117759466171\n",
      "Training Loss: 0.08456455916166306\n",
      "Training Loss: 0.10490046441555023\n",
      "Training Loss: 0.0926138237118721\n",
      "Training Loss: 0.10522711277008057\n",
      "Training Loss: 0.07775278389453888\n",
      "Training Loss: 0.06495654582977295\n",
      "Training Loss: 0.083611398935318\n",
      "Training Loss: 0.08409418910741806\n",
      "Training Loss: 0.1142306700348854\n",
      "Training Loss: 0.10364267230033875\n",
      "Training Loss: 0.1072826087474823\n",
      "Training Loss: 0.0965748280286789\n",
      "Training Loss: 0.11306612938642502\n",
      "Training Loss: 0.06670255213975906\n",
      "Training Loss: 0.08275806158781052\n",
      "Training Loss: 0.10332074016332626\n",
      "Training Loss: 0.09517519921064377\n",
      "Training Loss: 0.07806426286697388\n",
      "Training Loss: 0.10645129531621933\n",
      "Training Loss: 0.10416842997074127\n",
      "Training Loss: 0.09808812290430069\n",
      "Training Loss: 0.08502545207738876\n",
      "Training Loss: 0.11065950244665146\n",
      "Training Loss: 0.09569422155618668\n",
      "Training Loss: 0.06734191626310349\n",
      "Training Loss: 0.08610828965902328\n",
      "Training Loss: 0.09748323261737823\n",
      "Training Loss: 0.10385268181562424\n",
      "Training Loss: 0.08979654312133789\n",
      "Training Loss: 0.09378930926322937\n",
      "Training Loss: 0.09716181457042694\n",
      "Training Loss: 0.09527626633644104\n",
      "Training Loss: 0.10571389645338058\n",
      "Training Loss: 0.0680452212691307\n",
      "Training Loss: 0.08487847447395325\n",
      "Training Loss: 0.08737296611070633\n",
      "Training Loss: 0.10813576728105545\n",
      "Training Loss: 0.0794999748468399\n",
      "Training Loss: 0.09531054645776749\n",
      "Training Loss: 0.1009347140789032\n",
      "Training Loss: 0.12137842178344727\n",
      "Training Loss: 0.09181568771600723\n",
      "Training Loss: 0.09684046357870102\n",
      "Training Loss: 0.12654612958431244\n",
      "Training Loss: 0.09965313225984573\n",
      "Training Loss: 0.10332576930522919\n",
      "Training Loss: 0.0803995281457901\n",
      "Training Loss: 0.0765528455376625\n",
      "Training Loss: 0.11447140574455261\n",
      "Training Loss: 0.09872642159461975\n",
      "Training Loss: 0.07566627860069275\n",
      "Training Loss: 0.10321042686700821\n",
      "Training Loss: 0.09239842742681503\n",
      "Training Loss: 0.09907879680395126\n",
      "Training Loss: 0.12689167261123657\n",
      "Training Loss: 0.09898139536380768\n",
      "Training Loss: 0.1113179549574852\n",
      "Training Loss: 0.08364526182413101\n",
      "Training Loss: 0.09612523764371872\n",
      "Training Loss: 0.11677803099155426\n",
      "Training Loss: 0.1136639416217804\n",
      "Training Loss: 0.07396777719259262\n",
      "Training Loss: 0.10085765272378922\n",
      "Training Loss: 0.09461775422096252\n",
      "Training Loss: 0.08314207941293716\n",
      "Training Loss: 0.0970376506447792\n",
      "Training Loss: 0.11385330557823181\n",
      "Training Loss: 0.09982442855834961\n",
      "Training Loss: 0.07693547755479813\n",
      "Training Loss: 0.07999871671199799\n",
      "Training Loss: 0.08723774552345276\n",
      "Training Loss: 0.06875009089708328\n",
      "Training Loss: 0.08802790939807892\n",
      "Training Loss: 0.09403607249259949\n",
      "Training Loss: 0.06556826829910278\n",
      "Training Loss: 0.0887245461344719\n",
      "Training Loss: 0.08456826955080032\n",
      "Training Loss: 0.07540462911128998\n",
      "Training Loss: 0.06206412985920906\n",
      "Training Loss: 0.06474483758211136\n",
      "Training Loss: 0.07662753015756607\n",
      "Training Loss: 0.10051358491182327\n",
      "Training Loss: 0.08624864369630814\n",
      "Training Loss: 0.08546333760023117\n",
      "Training Loss: 0.07439754158258438\n",
      "Training Loss: 0.08644414693117142\n",
      "Training Loss: 0.11587455868721008\n",
      "Training Loss: 0.0968867838382721\n",
      "Training Loss: 0.05174899473786354\n",
      "Training Loss: 0.09764859825372696\n",
      "Training Loss: 0.060929372906684875\n",
      "Training Loss: 0.09395667910575867\n",
      "Training Loss: 0.07926471531391144\n",
      "Training Loss: 0.08945806324481964\n",
      "Training Loss: 0.09235064685344696\n",
      "Training Loss: 0.08713696151971817\n",
      "Training Loss: 0.07165306061506271\n",
      "Training Loss: 0.08107053488492966\n",
      "Training Loss: 0.13706541061401367\n",
      "Training Loss: 0.09451339393854141\n",
      "Training Loss: 0.128147691488266\n",
      "Training Loss: 0.10665582865476608\n",
      "Training Loss: 0.07553795725107193\n",
      "Training Loss: 0.10788147896528244\n",
      "Training Loss: 0.1201339140534401\n",
      "Training Loss: 0.11594606935977936\n",
      "Training Loss: 0.11577469855546951\n",
      "Training Loss: 0.111564040184021\n",
      "Training Loss: 0.08585760742425919\n",
      "Training Loss: 0.0785963237285614\n",
      "Training Loss: 0.08251588046550751\n",
      "Training Loss: 0.10688095539808273\n",
      "Training Loss: 0.06525304913520813\n",
      "Training Loss: 0.11505518853664398\n",
      "Training Loss: 0.09575586020946503\n",
      "Training Loss: 0.0977664664387703\n",
      "Training Loss: 0.11137643456459045\n",
      "Training Loss: 0.09296004474163055\n",
      "Training Loss: 0.0894390270113945\n",
      "Training Loss: 0.11400982737541199\n",
      "Training Loss: 0.10207310318946838\n",
      "Training Loss: 0.07364922016859055\n",
      "Training Loss: 0.06923291087150574\n",
      "Training Loss: 0.09422412514686584\n",
      "Training Loss: 0.07388552278280258\n",
      "Training Loss: 0.08637609332799911\n",
      "Training Loss: 0.10025947540998459\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "epochs = 30\n",
    "\n",
    "model = TicTacToeModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batched_x, batched_y in train_dataloader:\n",
    "        logits, loss = model(batched_x, batched_y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Training Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(30):\n",
    "    for xenc_batch, y_batch, lengths in dataloader:\n",
    "\n",
    "        # Move batch to GPU\n",
    "        xenc_batch = xenc_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = xenc_batch @ W  # Log-counts, shape: [batch_size, max_seq_len, alphabet_size]\n",
    " \n",
    "        logits = logits.view(-1, logits.size(-1))  # Shape: [batch_size * max_seq_len, vocab_size]\n",
    "        y_batch = y_batch.view(-1)  # Shape: [batch_size * max_seq_len]\n",
    "\n",
    "        # Compute the loss using CrossEntropyLoss\n",
    "        loss = F.cross_entropy(logits, y_batch, ignore_index=ctoi['.'])\n",
    "        \n",
    "        # Backward pass\n",
    "        W.grad = None  # Set gradients to zero before backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights (gradient descent)\n",
    "        with torch.no_grad():  # Don't track this update in the graph\n",
    "            W -= 5 * W.grad  # Gradient update\n",
    "        xloss.append(loss.item())\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
